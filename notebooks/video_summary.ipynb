{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-1",
   "metadata": {},
   "source": [
    "# Multimodal Speech, Slide Summarisation and Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-2",
   "metadata": {},
   "source": [
    "This notebook documents the end to end pipeline used in this project to transform a\n",
    "video (or any long technical talk) into:\n",
    "\n",
    "- a time stamped transcript generated from the audio track,\n",
    "- text extracted from the slides that appear in the video,\n",
    "- an aligned representation that links speech segments to the most relevant slide text,\n",
    "- one global abstractive summary suitable for fast review,\n",
    "- a semantic search index over the aligned segments, ready to be used in a RAG style workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proj-config-1",
   "metadata": {},
   "source": [
    "## Project configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proj-config-2",
   "metadata": {},
   "source": [
    "The implementation is intentionally modular. Each processing stage is implemented in a separate\n",
    "module and uses a small number of typed data structures to pass information to the\n",
    "next stage. The notebook provides a thin orchestration layer around these modules and can be\n",
    "used as both documentation and a reproducible demonstration of the pipeline.\n",
    "\n",
    "The pipeline has five core stages, each implemented by a dedicated Python module under `src/`:\n",
    "\n",
    "1. **Ingest (`ingest.py`)**: Inspect the input video container, extract the raw audio track, and sample frames at regular intervals using `moviepy` and `PIL`.\n",
    "\n",
    "2. **ASR (`asr.py`)**: Run automatic speech recognition (`faster-whisper`) on the audio track to obtain a time stamped transcript, returned as a list of `TranscriptSegment` objects with precise start and end times (in seconds) and the recognised text.\n",
    "\n",
    "3. **OCR (`ocr.py`)**: Run OCR with `pytesseract` on the sampled frames and return one `OCRRecord` per frame, containing the frame name, approximate timestamp, and the recognised text.\n",
    "\n",
    "4. **Alignment (`align.py`)**: Align the ASR and OCR streams and produce a list of `Segment` objects that carry both speech and slide text, with an explicit link to the frame used.\n",
    "\n",
    "5. **Summarisation (`summarise.py`)**: Use a transformer model from `transformers` to compress the concatenated segment texts into a short abstractive global summary.\n",
    "\n",
    "On top of these, an additional stage builds an embedding based semantic index:\n",
    "\n",
    "6. **Embeddings and semantic search (`embeddings.py`, `utils.py`)**: Compute sentence embeddings for each aligned segment, construct a `SemanticIndex`, and run nearest neighbour queries for interactive exploration or retrieval augmented generation.\n",
    "\n",
    "In addition, `models.py` defines the data structures exchanged between stages and helper functions for serialisation into JSON. Keeping these concerns separated makes it straightforward to reuse the notebook with alternative implementations (for example a different ASR backend or a different summarisation model) by changing only the internals of the corresponding module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*IProgress not found.*\")\n",
    "\n",
    "nb_root = os.getcwd()\n",
    "project_root = os.path.dirname(nb_root)\n",
    "src_dir = os.path.join(project_root, \"src\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.ingest import inspect_video, extract_audio, extract_frames\n",
    "from src.asr import run_asr, preview_transcript\n",
    "from src.ocr import run_ocr_on_frames, preview_ocr\n",
    "from src.align import align_transcript_and_ocr, preview_segments\n",
    "from src.summarise import summarise_segments\n",
    "\n",
    "from src.embeddings import EmbeddingConfig, load_embedding_model, build_index_from_output_dir\n",
    "from src.utils import (\n",
    "    load_segments,\n",
    "    pretty_print_results,\n",
    "    semantic_search,\n",
    "    get_segment_time_range,\n",
    "    get_segment_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datamodel-1",
   "metadata": {},
   "source": [
    "## Data model and JSON serialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datamodel-2",
   "metadata": {},
   "source": [
    "The pipeline exchanges data between stages using a small set of dataclasses defined in\n",
    "`src/models.py`:\n",
    "\n",
    "- `TranscriptSegment` with fields such as `start`, `end`, `text`.\n",
    "- `OCRRecord` with fields `time`, `frame`, `text`.\n",
    "- `Segment` that combines speech and slide information with fields such as\n",
    "  `start`, `end`, `speech`, `slide_text`, `slide_time`, `slide_frame`.\n",
    "\n",
    "Each dataclass exposes:\n",
    "\n",
    "- a `to_dict` method,\n",
    "- a `from_dict` static constructor,\n",
    "\n",
    "and there are helper functions such as `transcript_to_jsonable`, `ocr_to_jsonable` and\n",
    "`segments_to_jsonable` that convert lists of objects to standard lists of dictionaries.\n",
    "\n",
    "This design means that all intermediate results can be stored as human readable JSON files:\n",
    "\n",
    "- `transcript.json` is a list of transcript segments in chronological order.\n",
    "- `ocr.json` is a list of OCR records corresponding to processed frames.\n",
    "- `segments.json` is a list of aligned multimodal segments.\n",
    "- `summary.json` summarises the whole pipeline.\n",
    "\n",
    "Persisting these artefacts has several practical benefits:\n",
    "\n",
    "- experiments can be resumed from intermediate stages without recomputing heavy steps\n",
    "  such as ASR and OCR,\n",
    "- debugging is easier because each stage can be inspected individually,\n",
    "- training data for downstream models can be built from these JSON files without requiring\n",
    "  access to the original video or audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "paths-config-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root data directories\n",
    "data_dir = os.path.join(project_root, \"data\")\n",
    "interim_dir = os.path.join(data_dir, \"interim\")\n",
    "\n",
    "# Input video to analyse (update this to your own file)\n",
    "video_path = os.path.join(data_dir, \"input.mp4\")\n",
    "\n",
    "# Intermediate artefacts\n",
    "frame_dir = os.path.join(interim_dir, \"frames\")\n",
    "audio_path = os.path.join(interim_dir, \"audio.wav\")\n",
    "transcript_path = os.path.join(interim_dir, \"transcript.json\")\n",
    "ocr_output_path = os.path.join(interim_dir, \"ocr.json\")\n",
    "segments_path = os.path.join(interim_dir, \"segments.json\")\n",
    "\n",
    "for d in [data_dir, interim_dir, frame_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Frame extraction and OCR\n",
    "frame_interval_seconds = 3  # distance between extracted frames in seconds\n",
    "ocr_frame_stride = 2        # process every Nth frame for OCR to reduce cost\n",
    "\n",
    "# ASR configuration for faster-whisper\n",
    "asr_model_size = \"small\"       # for example: \"tiny\", \"base\", \"small\", \"medium\", \"large-v2\"\n",
    "asr_device = \"cuda\"            # \"cuda\" or \"cpu\"\n",
    "asr_compute_type = \"int8\"      # quantisation used by faster-whisper, see its documentation\n",
    "\n",
    "# Summarisation configuration\n",
    "summary_model_name = \"facebook/bart-large-cnn\"\n",
    "summary_device = 0            # GPU index, or -1 for CPU\n",
    "summary_max_chars = 3000      # max number of characters per chunk of transcript\n",
    "summary_max_length = 500      # max number of tokens in the summary of one chunk\n",
    "summary_min_length = 40       # min number of tokens in the summary of one chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirs-explain-1",
   "metadata": {},
   "source": [
    "The cell above creates the expected directory structure if it does not already exist.\n",
    "\n",
    "The convention used here is:\n",
    "\n",
    "- input video files live under `data/`,\n",
    "- heavy intermediate artefacts are stored in `data/interim/`,\n",
    "- additional cleaned datasets, if any, can be stored in `data/processed/` by other notebooks.\n",
    "\n",
    "The frame directory is created under `data/interim/frames/`. It acts both as cache for already\n",
    "extracted frames and as the input source for the OCR stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-1",
   "metadata": {},
   "source": [
    "## 1. Inspect input video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-2",
   "metadata": {},
   "source": [
    "The `inspect_video` helper uses `moviepy` to open the input file and prints basic information,\n",
    "such as:\n",
    "\n",
    "- duration (seconds),\n",
    "- resolution,\n",
    "- frame rate,\n",
    "- file size.\n",
    "\n",
    "This serves mainly as a sanity check that the file is readable and helps to choose reasonable\n",
    "values for frame sampling (for example, how many frames will be produced with the current\n",
    "`frame_interval_seconds` setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inspect-3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'input.mp4',\n",
       " 'path': 'C:\\\\Users\\\\kevin\\\\OneDrive\\\\Documents\\\\Work\\\\Python\\\\NLP-Videos\\\\data\\\\input.mp4',\n",
       " 'size_mb': 93.71,\n",
       " 'duration_seconds': 1075.39,\n",
       " 'duration_minutes': 17.92,\n",
       " 'fps': 60.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect_video(str(video_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ingest-1",
   "metadata": {},
   "source": [
    "## 2. Extract audio and frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ingest-2",
   "metadata": {},
   "source": [
    "The ingest stage performs two operations:\n",
    "\n",
    "1. **Audio extraction**: The `extract_audio` function opens the input video with `moviepy`, extracts the audio track and writes it to a `wav` file. The code is careful to avoid recomputation when the file\n",
    "   already exists on disk. This audio file is the only input required by the ASR stage.\n",
    "\n",
    "2. **Frame sampling**: The `extract_frames` function iterates over the video at a fixed temporal interval (`frame_interval_seconds`) and saves each sampled frame to the `frame_dir` folder as a JPEG\n",
    "   image. File names encode the time index (for example `frame_000120.jpg` for a frame taken\n",
    "   at t approximately 120 s).\n",
    "\n",
    "The function also exposes a light preview mode that prints how many frames were written\n",
    "and lists the first few frame names. This is useful to verify that the sampling frequency\n",
    "is appropriate for the type of video being processed.\n",
    "\n",
    "Both steps are idempotent: running the cell multiple times is safe and will not corrupt\n",
    "the existing artefacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ingest-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file already exists: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\audio.wav\n",
      "Planned number of frames: 359\n",
      "Saving frames to: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 359/359 [00:54<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio present: True\n",
      "Number of frame files: 359\n",
      "First few frame files:\n",
      " - frame_00000.jpg\n",
      " - frame_00001.jpg\n",
      " - frame_00002.jpg\n",
      " - frame_00003.jpg\n",
      " - frame_00004.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_audio(str(video_path), str(audio_path))\n",
    "\n",
    "extract_frames(\n",
    "    video_path=str(video_path),\n",
    "    frame_dir=str(frame_dir),\n",
    "    interval_seconds=frame_interval_seconds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asr-1",
   "metadata": {},
   "source": [
    "## 3. Automatic speech recognition (ASR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asr-2",
   "metadata": {},
   "source": [
    "The ASR stage converts raw audio into a list of time stamped transcript segments. The\n",
    "implementation in `src/asr.py` is based on `faster-whisper`, which provides a fast and memory\n",
    "efficient inference wrapper around OpenAI Whisper models.\n",
    "\n",
    "Main characteristics of `run_asr`:\n",
    "\n",
    "- Loads the specified Whisper checkpoint (for example `\"small\"` or `\"medium\"`).\n",
    "- Runs inference on the `audio_path` in streaming mode and collects the segments emitted by\n",
    "  Whisper. Each segment has a start time, an end time and a text field.\n",
    "- Wraps each raw segment into a `TranscriptSegment` dataclass instance.\n",
    "- Writes the full list of segments to `transcript_path` as JSON, using the helper\n",
    "  `transcript_to_jsonable` defined in `models.py`.\n",
    "- Prints the number of segments produced and the location of the JSON file.\n",
    "\n",
    "The choice of `asr_model_size`, `asr_device` and `asr_compute_type` controls the trade off\n",
    "between speed, resource usage and recognition quality.\n",
    "\n",
    "The helper `preview_transcript` is provided as a light inspection tool. It prints the first `n`\n",
    "segments with human readable timestamps and is designed to be called from notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "asr-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript file already exists: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\transcript.json\n",
      "Loaded 265 transcript segments.\n",
      "Total segments: 265\n",
      "[0.00 -> 6.04] This is a 3.\n",
      "[6.04 -> 11.52] It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain\n",
      "[11.52 -> 14.34] has no trouble recognizing it as a 3.\n",
      "[14.34 -> 18.52] And I want you to take a moment to appreciate how crazy it is that brains can do this so\n",
      "[18.52 -> 19.52] effortlessly.\n",
      "[19.52 -> 25.08] I mean, this, this, and this are also recognizable as 3s, even though the specific values\n",
      "[25.08 -> 28.84] of each pixel is very different from one image to the next.\n",
      "[28.84 -> 34.16] The particular light-sensitive cells in your eye that are firing when you see this 3 are\n",
      "[34.16 -> 37.60] very different from the ones firing when you see this 3.\n",
      "[37.60 -> 43.00] But something in that crazy smart visual cortex of yours resolves these as representing the\n"
     ]
    }
   ],
   "source": [
    "transcript_segments = run_asr(\n",
    "    audio_path=str(audio_path),\n",
    "    transcript_path=str(transcript_path),\n",
    "    model_size=asr_model_size,\n",
    "    device=asr_device,\n",
    "    compute_type=asr_compute_type,\n",
    ")\n",
    "\n",
    "preview_transcript(transcript_segments, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ocr-1",
   "metadata": {},
   "source": [
    "## 4. Optical character recognition (OCR) on frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ocr-2",
   "metadata": {},
   "source": [
    "The OCR stage recovers slide text from the video frames sampled during ingestion. It operates\n",
    "on the images stored in `frame_dir` and proceeds as follows:\n",
    "\n",
    "- Lists all frame files that match the expected naming pattern (for example\n",
    "  `frame_000120.jpg`). Each file name encodes the approximate time at which the frame was\n",
    "  captured.\n",
    "- Iterates over the frames in chronological order and processes only every\n",
    "  `ocr_frame_stride` th frame in order to keep the total OCR cost manageable.\n",
    "- For each selected frame:\n",
    "  - Opens the file with `PIL.Image`.\n",
    "  - Runs OCR using `pytesseract.image_to_string` with a configuration tuned for slide text.\n",
    "  - Constructs an `OCRRecord` object with the estimated timestamp (derived from the frame\n",
    "    index and `frame_interval_seconds`), the frame file name and the recognised text.\n",
    "\n",
    "The list of `OCRRecord` instances is serialised to `ocr_output_path` as JSON using the helper\n",
    "`ocr_to_jsonable`. The `preview_ocr` function prints the first records in a readable form\n",
    "including the approximate time and the corresponding frame file.\n",
    "\n",
    "The downstream alignment step only relies on the timestamps and the extracted text. If OCR\n",
    "quality is low for a particular video, the sampling frequency and OCR configuration can be\n",
    "tuned independently of the rest of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ocr-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR output already exists: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\ocr.json\n",
      "Loaded OCR text for 180 frames.\n",
      "OCR records: 180\n",
      "[t ~ 0.0s] frame=frame_00000.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 6.0s] frame=frame_00002.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 12.0s] frame=frame_00004.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 18.0s] frame=frame_00006.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 24.0s] frame=frame_00008.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 30.0s] frame=frame_00010.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 36.0s] frame=frame_00012.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 42.0s] frame=frame_00014.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 48.0s] frame=frame_00016.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 54.0s] frame=frame_00018.jpg\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0c\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 C0 Ce Ce OC\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.001 02) FRU ER RRB R iss 0.0 0.0 0.0 Ce ee ce ee\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2) TURES RS ORR BABA 01 Cc Ce ee ee oC\n",
      "PA O.7 1.0 1010 1.0 1.01010 10101010 Lo eta ag\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) pRB Rte 50) 9) 0.7 10 10 LO Leela\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1 0.1 0.1 0.0 0.0 0.0 0.0 OPERA Ries re 0 0.0 00 00 00\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 OSE RRIER NCL C0 0.0 00 00 TC\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 (XR RIRRURRY C1 CC 0.0 0 00 80 RC\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 ESR SRR BRET CC CC OC £0 00 DC TO\n",
      "\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1) CURR R RRR) C5 0.0 0-0 00 0.0 1.0 00 D0\n",
      "\n",
      "POR RES et] 0.9 1.0 1.0 1.0 1.0 1.0 LC LO LC 0.7 TS i a nat\n",
      "ORR es 0.9 1.0 1.01.0 LO 1.0 LC LC 1.0 1.0 i aan any\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 01/7 02 02 C2 C2 0.2 irk AI\n",
      "0.0 6.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 C0 G0 C.0 CC CC C7 PE cm on op aD ra dood oo. a\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 CC CC C0 CC CC OP TRIBE D® 0.0 0.0 D0 OD BO OO\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 CC CC OC 0.0 CC RRR 0% 0.0 0.0 80 00 0\n",
      "\n",
      "0.0 0.0 0.0 0.0 0.0 0.2 [gr RY) 01 C0 CC C0 C0 05 hs BRD 0.9 0.0 0.0 OO oo\n",
      "\n",
      "0.0 0.0 0.0 0.0 0.0 [Ee Ree PP C8 I0 101-0 0” Aa\n",
      "PRY 0.8 LO LO LO LO IC Ie Le 10 1-0 Cy 0 NS\n",
      "\n",
      "LOD AO Me hd dat\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ocr_records = run_ocr_on_frames(\n",
    "    frame_dir=str(frame_dir),\n",
    "    ocr_output_path=str(ocr_output_path),\n",
    "    frame_interval_seconds=frame_interval_seconds,\n",
    "    ocr_frame_stride=ocr_frame_stride,\n",
    ")\n",
    "\n",
    "preview_ocr(ocr_records, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "align-1",
   "metadata": {},
   "source": [
    "## 5. Align transcript and slide text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "align-2",
   "metadata": {},
   "source": [
    "The goal of the alignment stage is to attach the most relevant slide text to each speech\n",
    "segment. The implementation in `src/align.py` performs a simple, but effective, temporal\n",
    "nearest neighbour matching.\n",
    "\n",
    "High level algorithm of `align_transcript_and_ocr`:\n",
    "\n",
    "1. Precompute a list of OCR times (one value per `OCRRecord`).\n",
    "2. For each `TranscriptSegment` in chronological order:\n",
    "   - locate the index of the nearest OCR time,\n",
    "   - copy the corresponding slide text and frame information into a new `Segment` instance.\n",
    "3. Optionally apply light post processing that merges very short segments and removes borderline\n",
    "   duplicates in order to obtain smoother segments.\n",
    "\n",
    "Each resulting `Segment` therefore contains:\n",
    "\n",
    "- the start and end time of the speech segment,\n",
    "- the speech text as produced by ASR,\n",
    "- the slide text and corresponding slide timestamp,\n",
    "- the name of the frame from which the slide text was extracted.\n",
    "\n",
    "The list of segments is saved to `segments_path` as JSON to make it easy to inspect or reuse\n",
    "in downstream tasks.\n",
    "\n",
    "The helper `preview_segments` prints the first `n` aligned segments, showing both the speech\n",
    "and the associated slide text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "align-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aligned multimodal segments to: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\segments.json\n",
      "Total segments: 265\n",
      "Aligned segments: 265\n",
      "[0.00 -> 6.04] (slide at 6.0) speech='This is a 3....'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[6.04 -> 11.52] (slide at 6.0) speech='It's sloppily written and rendered at an extremely low resol...'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[11.52 -> 14.34] (slide at 12.0) speech='has no trouble recognizing it as a 3....'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[14.34 -> 18.52] (slide at 18.0) speech='And I want you to take a moment to appreciate how crazy it i...'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[18.52 -> 19.52] (slide at 18.0) speech='effortlessly....'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[19.52 -> 25.08] (slide at 24.0) speech='I mean, this, this, and this are also recognizable as 3s, ev...'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[25.08 -> 28.84] (slide at 24.0) speech='of each pixel is very different from one image to the next....'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[28.84 -> 34.16] (slide at 30.0) speech='The particular light-sensitive cells in your eye that are fi...'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[34.16 -> 37.60] (slide at 36.0) speech='very different from the ones firing when you see this 3....'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[37.60 -> 43.00] (slide at 42.0) speech='But something in that crazy smart visual cortex of yours res...'\n",
      "Slide text: \n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "segments = align_transcript_and_ocr(\n",
    "    transcript_segments=transcript_segments,\n",
    "    ocr_records=ocr_records,\n",
    "    segments_path=str(segments_path),\n",
    ")\n",
    "\n",
    "preview_segments(segments, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-1",
   "metadata": {},
   "source": [
    "## 6. Global abstractive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-2",
   "metadata": {},
   "source": [
    "The final stage compresses the aligned transcript into a short textual summary. The function\n",
    "`summarise_segments` in `src/summarise.py` uses the following strategy:\n",
    "\n",
    "1. Concatenate the `speech` fields of all segments into a single long transcript string.\n",
    "2. Split this string into chunks using `_chunk_text`, which respects sentence boundaries\n",
    "   when possible and ensures that each chunk does not exceed `max_chunk_chars` characters.\n",
    "3. Instantiate a Hugging Face `pipeline` with a summarisation model (by default\n",
    "   `\"facebook/bart-large-cnn\"`) on the requested device (GPU index or CPU).\n",
    "4. For each chunk:\n",
    "   - call the summariser with `max_length` and `min_length` constraints,\n",
    "   - collect the resulting partial summary text.\n",
    "5. Concatenate all chunk level summaries into one global summary string.\n",
    "\n",
    "The chunking mechanism is important for two reasons:\n",
    "\n",
    "- it allows the use of standard encoder decoder models with relatively short maximum input\n",
    "  lengths on arbitrarily long transcripts,\n",
    "- it constrains the worst case memory usage during generation.\n",
    "\n",
    "`summarise_segments` prints the global summary to standard output and writes a structured\n",
    "`VideoSummary` object as `summary.json` under the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "summary-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transcript length (characters): 17906\n",
      "Number of chunks for summarisation: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising chunk 1/7 …\n",
      "Summarising chunk 2/7 …\n",
      "Summarising chunk 3/7 …\n",
      "Summarising chunk 4/7 …\n",
      "Summarising chunk 5/7 …\n",
      "Summarising chunk 6/7 …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 500, but your input_length is only 57. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising chunk 7/7 …\n",
      "\n",
      "=== GLOBAL SUMMARY ===\n",
      "\n",
      "In this video, we look at how a neural network can learn to recognize handwritten digits. In the next, we'll look at the structure component of that. At the end of the two videos, I want to point you to a couple of good resources where you can learn more.\n",
      "The network starts with a bunch of neurons corresponding to each of the 28 times 28 pixels of the input image, which is 784 neurons in total. The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit.\n",
      "The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits. In a perfect world, we might hope that each neuron in the second to last layer of the network corresponds with one of these subcomponents.\n",
      "The question at hand is, what parameters should the network have? What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern? Well, what we'll do is assign a weight to each one of the connections between our neurons.\n",
      "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights and 16 biases. The connections between the other layers also have a bunch of weights and biasesassociated with them. This network has almost exactly 13,000 total weights and bias.\n",
      "The entire network is just a function. sigmoid function to each specific component of the resulting vector inside. Once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next.\n",
      "Using sigmoids didn't help training or it was very difficult to train at some point, and people just tried relu. simplification. And it happened to work very well for these incredibly deep neuralnetworks.\n",
      "Structured summary written to C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\summary.json\n"
     ]
    }
   ],
   "source": [
    "summary = summarise_segments(\n",
    "    segments=segments,\n",
    "    model_name=summary_model_name,\n",
    "    device=summary_device,\n",
    "    max_chunk_chars=summary_max_chars,\n",
    "    max_length=summary_max_length,\n",
    "    min_length=summary_min_length,\n",
    "    video_path=video_path,\n",
    "    output_dir=interim_dir,\n",
    "    summary_filename=\"summary.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings-intro-1",
   "metadata": {},
   "source": [
    "## 7. Segment embeddings and semantic search\n",
    "\n",
    "At this point, the pipeline has produced `segments.json` and `summary.json` under `data/interim/`.\n",
    "The segments file contains the aligned multimodal units that combine speech and slide text.\n",
    "\n",
    "The next step is to build a sentence embedding index over these segments so that free form\n",
    "queries can retrieve the most relevant parts of the talk. This section turns the aligned segments\n",
    "into a small RAG ready store using `sentence-transformers` and the `SemanticIndex` defined in\n",
    "`src/utils.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-segments-1",
   "metadata": {},
   "source": [
    "### 7.1 Inspect the segments file\n",
    "\n",
    "Inspecting a few raw segments clarifies the structure, for example:\n",
    "\n",
    "- which keys hold the transcript text,\n",
    "- how timestamps are represented,\n",
    "- whether slide text or OCR text is present.\n",
    "\n",
    "The semantic search index uses a combination of such text fields to build embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "inspect-segments-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 265 segments\n",
      "--- Segment 0 ---\n",
      "start: 0.0\n",
      "end: 6.04\n",
      "mid: 3.02\n",
      "speech: This is a 3.\n",
      "slide_text: \n",
      "slide_time: 6.0\n",
      "slide_frame: frame_00002.jpg\n",
      "\n",
      "--- Segment 1 ---\n",
      "start: 6.04\n",
      "end: 11.52\n",
      "mid: 8.78\n",
      "speech: It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain\n",
      "slide_text: \n",
      "slide_time: 6.0\n",
      "slide_frame: frame_00002.jpg\n",
      "\n",
      "--- Segment 2 ---\n",
      "start: 11.52\n",
      "end: 14.34\n",
      "mid: 12.93\n",
      "speech: has no trouble recognizing it as a 3.\n",
      "slide_text: \n",
      "slide_time: 12.0\n",
      "slide_frame: frame_00004.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "segments_records = load_segments(segments_path)\n",
    "print(f\"Loaded {len(segments_records)} segments\")\n",
    "\n",
    "for i, seg in enumerate(segments_records[:3]):\n",
    "    print(f\"--- Segment {i} ---\")\n",
    "    for key, value in seg.items():\n",
    "        if isinstance(value, str) and len(value) > 120:\n",
    "            value_display = value[:120] + \"...\"\n",
    "        else:\n",
    "            value_display = value\n",
    "        print(f\"{key}: {value_display}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-segments-3",
   "metadata": {},
   "source": [
    "Typical keys observed in the output might include:\n",
    "\n",
    "- `segment_id` or `id`,\n",
    "- `start` or `start_sec` and `end` or `end_sec`,\n",
    "- `text`, `transcript`, or `asr_text`,\n",
    "- `speech` (from the alignment stage),\n",
    "- `slide_text` or `ocr_text`.\n",
    "\n",
    "The helper function `build_segment_text` (used internally when computing embeddings) already\n",
    "handles common combinations of these fields, so small schema changes generally do not require\n",
    "modifications to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embed-config-1",
   "metadata": {},
   "source": [
    "### 7.2 Configure and load the embedding model\n",
    "\n",
    "`EmbeddingConfig` controls aspects such as:\n",
    "\n",
    "- the sentence-transformers model to use,\n",
    "- batch size,\n",
    "- device (`\"cpu\"` or `\"cuda\"`),\n",
    "- whether to apply L2 normalisation to embeddings,\n",
    "- the name of the embedding file written in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "embed-config-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingConfig(model_name='sentence-transformers/all-MiniLM-L12-v2', batch_size=32, device='cuda', normalize_embeddings=True, embeddings_filename='segment_embeddings.npy')\n",
      "Loaded embedding model: sentence-transformers/all-MiniLM-L12-v2\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "config = EmbeddingConfig(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "    batch_size=32,\n",
    "    device=\"cuda\",  # set to \"cpu\" if no GPU is available\n",
    "    normalize_embeddings=True,\n",
    "    embeddings_filename=\"segment_embeddings.npy\",\n",
    ")\n",
    "\n",
    "print(config)\n",
    "\n",
    "embed_model = load_embedding_model(config)\n",
    "print(f\"Loaded embedding model: {config.model_name}\")\n",
    "print(f\"Embedding dimension: {embed_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "index-build-1",
   "metadata": {},
   "source": [
    "### 7.3 Build or load the semantic search index\n",
    "\n",
    "The helper `build_index_from_output_dir` performs the following steps:\n",
    "\n",
    "1. Loads `segments.json` from the specified output directory.\n",
    "2. Loads `segment_embeddings.npy` if present.\n",
    "3. Otherwise computes embeddings and saves them to `segment_embeddings.npy`.\n",
    "4. Builds an in memory nearest neighbour index over the embeddings using `SemanticIndex`.\n",
    "\n",
    "After this step, semantic search queries operate directly on the in memory index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "index-build-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic search index ready.\n",
      "Number of segments indexed: 265\n"
     ]
    }
   ],
   "source": [
    "index = build_index_from_output_dir(output_dir=interim_dir, config=config, model=embed_model)\n",
    "\n",
    "print(\"Semantic search index ready.\")\n",
    "print(f\"Number of segments indexed: {len(index.segments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings-inspect-1",
   "metadata": {},
   "source": [
    "The raw embeddings array is available on disk and can be inspected directly if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "embeddings-inspect-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (265, 384)\n",
      "Sample embedding vector (first segment):\n",
      "[-0.03239828  0.01507453  0.06563838 -0.03154001  0.05292244 -0.04820523\n",
      "  0.0812026  -0.00355402  0.05419927 -0.03468784]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_path = os.path.join(interim_dir, config.embeddings_filename)\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(\"Sample embedding vector (first segment):\")\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-demo-1",
   "metadata": {},
   "source": [
    "### 7.4 Run example search queries\n",
    "\n",
    "Free form queries are now evaluated against the indexed segments:\n",
    "\n",
    "- the query is embedded with the same sentence-transformers model,\n",
    "- nearest neighbours are retrieved in the embedding space,\n",
    "- results are returned with similarity scores and segment metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "657c6e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does the network recognize handwritten digits?\n",
      "\n",
      "Rank 1 | score=0.429 | 89.32 s → 93.44 s | id=[no id]\n",
      "feel like you know what it means when you read or hear about a neural network quote\n",
      "\n",
      "[SLIDE]\n",
      "aji1 = 0(Wa; + 7) Machine learning\n",
      "O) Neural network\n",
      "\n",
      "ERA BN\n",
      "\n",
      "SIM SE WEA\n",
      "\n",
      "KS CPS\n",
      "7\n",
      "\n",
      "ARS\n",
      "SAW,\n",
      "\n",
      "N\n",
      "\n",
      "eo\n",
      "Ea\n",
      "\n",
      "MSN\n",
      "\n",
      "SK\n",
      "\n",
      "Why the layers?\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2 | score=0.429 | 85.04 s → 89.32 s | id=[no id]\n",
      "My hope is just that you come away feeling like the structure itself is motivated, and to\n",
      "\n",
      "[SLIDE]\n",
      "aji1 = 0(Wa; + 7) Machine learning\n",
      "O) Neural network\n",
      "\n",
      "ERA BN\n",
      "\n",
      "SIM SE WEA\n",
      "\n",
      "KS CPS\n",
      "7\n",
      "\n",
      "ARS\n",
      "SAW,\n",
      "\n",
      "N\n",
      "\n",
      "eo\n",
      "Ea\n",
      "\n",
      "MSN\n",
      "\n",
      "SK\n",
      "\n",
      "Why the layers?\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3 | score=0.427 | 169.04 s → 172.60 s | id=[no id]\n",
      "What are the neurons, and in what sense are they linked together?\n",
      "\n",
      "[SLIDE]\n",
      "Neural network\n",
      "\n",
      "What are\n",
      "\n",
      "the me uc so:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4 | score=0.427 | 167.52 s → 169.04 s | id=[no id]\n",
      "But let's break that down.\n",
      "\n",
      "[SLIDE]\n",
      "Neural network\n",
      "\n",
      "What are\n",
      "\n",
      "the me uc so:\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5 | score=0.427 | 163.68 s → 167.52 s | id=[no id]\n",
      "As the name suggests, neural networks are inspired by the brain.\n",
      "\n",
      "[SLIDE]\n",
      "Neural network\n",
      "\n",
      "What are\n",
      "\n",
      "the me uc so:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "semantic_search(index, embed_model,\n",
    "                \"How does the network recognize handwritten digits?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f265ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the role of the hidden layer in this example?\n",
      "\n",
      "Rank 1 | score=0.497 | 85.04 s → 89.32 s | id=[no id]\n",
      "My hope is just that you come away feeling like the structure itself is motivated, and to\n",
      "\n",
      "[SLIDE]\n",
      "aji1 = 0(Wa; + 7) Machine learning\n",
      "O) Neural network\n",
      "\n",
      "ERA BN\n",
      "\n",
      "SIM SE WEA\n",
      "\n",
      "KS CPS\n",
      "7\n",
      "\n",
      "ARS\n",
      "SAW,\n",
      "\n",
      "N\n",
      "\n",
      "eo\n",
      "Ea\n",
      "\n",
      "MSN\n",
      "\n",
      "SK\n",
      "\n",
      "Why the layers?\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2 | score=0.497 | 89.32 s → 93.44 s | id=[no id]\n",
      "feel like you know what it means when you read or hear about a neural network quote\n",
      "\n",
      "[SLIDE]\n",
      "aji1 = 0(Wa; + 7) Machine learning\n",
      "O) Neural network\n",
      "\n",
      "ERA BN\n",
      "\n",
      "SIM SE WEA\n",
      "\n",
      "KS CPS\n",
      "7\n",
      "\n",
      "ARS\n",
      "SAW,\n",
      "\n",
      "N\n",
      "\n",
      "eo\n",
      "Ea\n",
      "\n",
      "MSN\n",
      "\n",
      "SK\n",
      "\n",
      "Why the layers?\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3 | score=0.436 | 278.88 s → 283.52 s | id=[no id]\n",
      "And of course the heart of the network, as an information processing mechanism, comes\n",
      "\n",
      "[SLIDE]\n",
      "“Hidden layers”\n",
      "\n",
      "784\n",
      "\n",
      "EAS\n",
      "Sra\n",
      "LAL SES\n",
      "gn Oe\n",
      "\n",
      "O24\n",
      "\n",
      "LLG\n",
      "My)\n",
      "LE\n",
      "\n",
      "EY\n",
      "'S oe\n",
      "\n",
      "ORS\n",
      "ae\n",
      "\n",
      "YEN igs\n",
      "Wo) 8\n",
      "Uy\n",
      "\n",
      "£E\n",
      "\n",
      "eA\n",
      "ZF\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4 | score=0.416 | 344.28 s → 345.52 s | id=[no id]\n",
      "What are we expecting here?\n",
      "\n",
      "[SLIDE]\n",
      "784\n",
      "\n",
      "Why the layers?\n",
      "\n",
      "WSS\n",
      "Ws 0\n",
      "SSSRZ\n",
      "\n",
      "gS\n",
      "YES NN\n",
      "YSN\n",
      "\n",
      "Za\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5 | score=0.416 | 342.24 s → 344.28 s | id=[no id]\n",
      "behave intelligently.\n",
      "\n",
      "[SLIDE]\n",
      "784\n",
      "\n",
      "Why the layers?\n",
      "\n",
      "WSS\n",
      "Ws 0\n",
      "SSSRZ\n",
      "\n",
      "gS\n",
      "YES NN\n",
      "YSN\n",
      "\n",
      "Za\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "semantic_search(index, embed_model,\n",
    "                \"What is the role of the hidden layer in this example?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bceac201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How many parameters does the network have?\n",
      "\n",
      "Rank 1 | score=0.468 | 947.72 s → 951.96 s | id=[no id]\n",
      "It's an absurdly complicated function, one that involves 13,000 parameters\n",
      "\n",
      "[SLIDE]\n",
      "Network\n",
      "\n",
      "|\n",
      "Function\n",
      "\n",
      "NuUuMbpE\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2 | score=0.446 | 965.72 s → 969.96 s | id=[no id]\n",
      "looks complicated. I mean, if it were any simpler, what hope would we have that it\n",
      "\n",
      "[SLIDE]\n",
      "Network\n",
      "\n",
      "|\n",
      "Function\n",
      "\n",
      "o\n",
      "(ay\n",
      "a\n",
      "<\n",
      "co\n",
      "a)\n",
      "a\n",
      "Lo\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3 | score=0.446 | 960.84 s → 965.72 s | id=[no id]\n",
      "But it's just a function nonetheless. And in a way, it's kind of reassuring that it\n",
      "\n",
      "[SLIDE]\n",
      "Network\n",
      "\n",
      "|\n",
      "Function\n",
      "\n",
      "o\n",
      "(ay\n",
      "a\n",
      "<\n",
      "co\n",
      "a)\n",
      "a\n",
      "Lo\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4 | score=0.427 | 540.36 s → 545.24 s | id=[no id]\n",
      "to potentially capture this pattern, or any other pixel pattern, or the pattern\n",
      "\n",
      "[SLIDE]\n",
      "What parameters should exist?\n",
      "\n",
      "a 0.53\n",
      "\n",
      "P2: 0.29\n",
      ". P3: 0.02\n",
      "; ) Pa: 0.77\n",
      "\n",
      "ues ) Ps: 0.90\n",
      "Pe: 0.96\n",
      "7: 0.02\n",
      "Ps: 1.00\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5 | score=0.427 | 535.68 s → 540.36 s | id=[no id]\n",
      "What dials and knobs should you be able to tweak so that it's expressive enough\n",
      "\n",
      "[SLIDE]\n",
      "What parameters should exist?\n",
      "\n",
      "a 0.53\n",
      "\n",
      "P2: 0.29\n",
      ". P3: 0.02\n",
      "; ) Pa: 0.77\n",
      "\n",
      "ues ) Ps: 0.90\n",
      "Pe: 0.96\n",
      "7: 0.02\n",
      "Ps: 1.00\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "semantic_search(index, embed_model,\n",
    "                \"How many parameters does the network have?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7eb938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Why is the sigmoid activation function used?\n",
      "\n",
      "Rank 1 | score=0.657 | 841.96 s → 845.92 s | id=[no id]\n",
      "the first layer, according to these weights, corresponds to one of the terms\n",
      "\n",
      "[SLIDE]\n",
      "Sigmoid\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 2 | score=0.657 | 649.48 s → 654.60 s | id=[no id]\n",
      "So the activation of the neuron here is basically a measure of how positive\n",
      "\n",
      "[SLIDE]\n",
      "Sigmoid\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 3 | score=0.657 | 1059.48 s → 1065.00 s | id=[no id]\n",
      "simplification. Using sigmoids didn't help training or it was very difficult to train at some point,\n",
      "\n",
      "[SLIDE]\n",
      "Sigmoid\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 4 | score=0.657 | 654.60 s → 657.84 s | id=[no id]\n",
      "the relevant weighted sum is.\n",
      "\n",
      "[SLIDE]\n",
      "Sigmoid\n",
      "--------------------------------------------------------------------------------\n",
      "Rank 5 | score=0.570 | 632.56 s → 635.84 s | id=[no id]\n",
      "And a common function that does this is called the sigmoid function,\n",
      "\n",
      "[SLIDE]\n",
      "Sigmoid\n",
      "_ i\n",
      "~ l+e*\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "semantic_search(index, embed_model,\n",
    "                \"Why is the sigmoid activation function used?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "programmatic-1",
   "metadata": {},
   "source": [
    "### 7.5 Using search results programmatically\n",
    "\n",
    "In a RAG setup, retrieved segments usually serve as context for downstream question answering.\n",
    "The `search` method returns `SearchResult` instances that carry both similarity metadata and the\n",
    "original segment payload.\n",
    "\n",
    "The example below extracts timestamps and segment identifiers for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "programmatic-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw result fields: ['rank', 'score', 'text', 'segment', 'extra']\n",
      "\n",
      "Rank = 1, segment id = [no id], time = 487.92 s → 493.08 s, score = 0.461\n",
      "Parsing speech, for example, involves taking raw audio and picking out distinct sounds\n",
      "\n",
      "[SLIDE]\n",
      "Raw audio\n",
      "\n",
      "Rank = 2, segment id = [no id], time = 493.08 s → 497.52 s, score = 0.388\n",
      "which combine to make certain syllables, which combine to form words, which combine\n",
      "\n",
      "[SLIDE]\n",
      "t-te —> recognition — re-cog-ni-tion — recognition\n",
      "\n",
      "Raw audio\n",
      "\n",
      "titmt\n",
      "\n",
      "Rank = 3, segment id = [no id], time = 497.52 s → 501.32 s, score = 0.388\n",
      "to make up phrases and more abstract thoughts, etc.\n",
      "\n",
      "[SLIDE]\n",
      "t-te —> recognition — re-cog-ni-tion — recognition\n",
      "\n",
      "Raw audio\n",
      "\n",
      "titmt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How is audio aligned with slide text in the system?\"\n",
    "results = index.search(query=query, model=embed_model, top_k=3)\n",
    "\n",
    "print(f\"Raw result fields: {list(vars(results[0]).keys())}\\n\")\n",
    "\n",
    "for r in results:\n",
    "    seg = r.segment            # dict record\n",
    "    score = r.score            # float\n",
    "    rank = r.rank              # int\n",
    "\n",
    "    seg_id = seg.get(\"segment_id\") or seg.get(\"id\") or \"[no id]\"\n",
    "    time_range = get_segment_time_range(seg)\n",
    "    text = get_segment_text(seg)\n",
    "\n",
    "    snippet = text[:200] + (\"...\" if len(text) > 200 else \"\")\n",
    "    print(f\"Rank = {rank}, segment id = {seg_id}, time = {time_range}, score = {score:.3f}\")\n",
    "    print(snippet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "programmatic-3",
   "metadata": {},
   "source": [
    "The resulting structures typically contain enough information to:\n",
    "\n",
    "- link back to a video player through timestamps and segment identifiers,\n",
    "- pass relevant text fields to an LLM along with the user question,\n",
    "- build citation objects of the form `{ \"segment_id\": ..., \"start_sec\": ..., \"end_sec\": ... }`.\n",
    "\n",
    "This completes a basic bridge between the multimodal segment pipeline and a retrieval augmented\n",
    "        question answering system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the complete multimodal pipeline, starting from a raw\n",
    "video file and producing:\n",
    "\n",
    "- an audio transcript derived from the video soundtrack,\n",
    "- slide text extracted from sampled frames,\n",
    "- aligned multimodal segments that combine both modalities,\n",
    "- an abstractive summary suitable for fast review or indexing,\n",
    "- a semantic search index that turns these segments into a compact, queryable knowledge base.\n",
    "\n",
    "All heavy computation is delegated to the reusable modules under `src/`, while the notebook\n",
    "focuses on configuration, orchestration, and interactive exploration. This separation permits\n",
    "straightforward extension of the project with additional experiments, visualisations or\n",
    "alternative models in dedicated notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
