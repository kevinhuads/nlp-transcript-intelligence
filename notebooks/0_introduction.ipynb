{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e81e9d86",
   "metadata": {},
   "source": [
    "# Multimodal Speech and Slide Summarisation\n",
    "\n",
    "This notebook documents the end to end pipeline used in this project to transform a teaching\n",
    "video (or any long technical talk) into:\n",
    "\n",
    "- a time stamped transcript generated from the audio track,\n",
    "- text extracted from the slides that appear in the video,\n",
    "- an aligned representation that links speech segments to the most relevant slide text,\n",
    "- one global abstractive summary suitable for fast review.\n",
    "\n",
    "The implementation is intentionally modular. Each processing stage is implemented in a separate\n",
    "module under `src/` and uses a small number of typed data structures to pass information to the\n",
    "next stage. The notebook provides a thin orchestration layer around these modules and can be\n",
    "used as both documentation and a reproducible demonstration of the pipeline.\n",
    "\n",
    "The pipeline has five main stages:\n",
    "\n",
    "1. **Ingest**: inspect the video container and extract the raw audio track and sampled frames.\n",
    "2. **ASR**: run automatic speech recognition (ASR) on the audio track in order to obtain a\n",
    "   time stamped transcript.\n",
    "3. **OCR**: run optical character recognition (OCR) on sampled frames to recover slide text.\n",
    "4. **Alignment**: match each speech segment with the temporally closest slide text and build\n",
    "   rich multimodal segments.\n",
    "5. **Summarisation**: compress the entire transcript into a short abstractive summary with a\n",
    "   transformer based text summarisation model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a4957",
   "metadata": {},
   "source": [
    "## Project structure\n",
    "\n",
    "A typical layout for this repository is\n",
    "\n",
    "```text\n",
    "project_root/\n",
    "  data/\n",
    "    interim/    # heavy intermediate artefacts (frames, transcripts, OCR output)\n",
    "    processed/  # cleaned / final data used for modelling (optional)\n",
    "  src/\n",
    "    __init__.py\n",
    "    ingest.py\n",
    "    asr.py\n",
    "    ocr.py\n",
    "    align.py\n",
    "    summarise.py\n",
    "    models.py\n",
    "  notebooks/\n",
    "    0_introduction.ipynb\n",
    "\n",
    "```\n",
    "\n",
    "The goal of this introduction notebook is to live in the `notebooks/` folder and provide a\n",
    "compact, but technically detailed, walkthrough of the full multimodal pipeline. The more\n",
    "specialised notebooks can focus on exploratory data analysis, model comparison or MLOps\n",
    "concerns without having to repeat the core logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5647ab",
   "metadata": {},
   "source": [
    "## Environment and imports\n",
    "\n",
    "The following cell tries to infer the project root from the current working directory. The\n",
    "assumption is that:\n",
    "\n",
    "- you open the notebook from inside the `notebooks/` directory, or\n",
    "- you open it from the project root.\n",
    "\n",
    "In both cases we derive the root path and register it on `sys.path` so that the `src.*` modules\n",
    "can be imported without any additional configuration. If your tree is different, you only need\n",
    "to adjust the definition of `project_root`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b099997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\notebooks\n",
      "Project root:       C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\n",
      "Source directory:   C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Current notebook directory\n",
    "nb_root = os.getcwd()\n",
    "\n",
    "# Infer project root from the notebook location\n",
    "if os.path.basename(nb_root) == \"notebooks\":\n",
    "    project_root = os.path.dirname(nb_root)\n",
    "else:\n",
    "    project_root = nb_root\n",
    "\n",
    "# Conventional source directory\n",
    "src_dir = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Register project root on sys.path so that `import src.*` works in-place\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(f\"Notebook directory: {nb_root}\")\n",
    "print(f\"Project root:       {project_root}\")\n",
    "print(f\"Source directory:   {src_dir}\")\n",
    "\n",
    "if not os.path.exists(src_dir):\n",
    "    print(\"Warning: src directory not found. Adjust project_root if needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e245d864",
   "metadata": {},
   "source": [
    "## Import project modules\n",
    "\n",
    "The core functionality of the pipeline is implemented as plain Python modules under `src/`:\n",
    "\n",
    "- `ingest.py` exposes utilities to inspect the input video, extract its audio track and sample\n",
    "  frames at regular intervals using `moviepy` and `PIL`.\n",
    "- `asr.py` wraps a `faster-whisper` model and returns a list of `TranscriptSegment` objects\n",
    "  with precise start and end times (seconds) and the recognised text.\n",
    "- `ocr.py` runs OCR with `pytesseract` on sampled frames and returns one `OCRRecord` per frame,\n",
    "  containing the frame name, the approximate timestamp and the recognised text.\n",
    "- `align.py` aligns the two previous streams and produces a list of `Segment` objects that\n",
    "  carry both speech and slide text, with an explicit link to the frame used.\n",
    "- `summarise.py` uses a transformer model from `transformers` to generate a global abstract\n",
    "  summary from the concatenated segment texts.\n",
    "- `models.py` defines the data structures used between the stages, and helper functions for\n",
    "  serialisation into JSON.\n",
    "\n",
    "Keeping these concerns separated makes it straightforward to reuse the notebook with alternative\n",
    "implementations (for example a different ASR backend or a different summarisation model) by\n",
    "changing only the internals of the corresponding module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c30b9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "from src.ingest import inspect_video, extract_audio, extract_frames\n",
    "from src.asr import run_asr, preview_transcript\n",
    "from src.ocr import run_ocr_on_frames, preview_ocr\n",
    "from src.align import align_transcript_and_ocr, preview_segments\n",
    "from src.summarise import summarise_segments\n",
    "\n",
    "print(\"Project modules imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2debc",
   "metadata": {},
   "source": [
    "## Data model and JSON serialisation\n",
    "\n",
    "The pipeline exchanges data between stages using a small set of dataclasses defined in\n",
    "`src/models.py`:\n",
    "\n",
    "- `TranscriptSegment` with fields `start`, `end`, `text`.\n",
    "- `OCRRecord` with fields `time`, `frame`, `text`.\n",
    "- `Segment` that combines speech and slide information with fields such as\n",
    "  `start`, `end`, `speech`, `slide_text`, `slide_time`, `slide_frame`.\n",
    "\n",
    "Each dataclass exposes:\n",
    "\n",
    "- a `to_dict` method,\n",
    "- a `from_dict` static constructor,\n",
    "\n",
    "and there are helper functions such as `transcript_to_jsonable`, `ocr_to_jsonable` and\n",
    "`segments_to_jsonable` that convert lists of objects to standard lists of dictionaries.\n",
    "\n",
    "This design means that all intermediate results can be stored as human readable JSON files:\n",
    "\n",
    "- `transcript.json` is a list of transcript segments in chronological order.\n",
    "- `ocr.json` is a list of OCR records corresponding to processed frames.\n",
    "- `segments.json` is a list of aligned multimodal segments.\n",
    "\n",
    "Persisting these artefacts has several practical benefits:\n",
    "\n",
    "- experiments can be resumed from intermediate stages without recomputing heavy steps\n",
    "  such as ASR and OCR,\n",
    "- debugging is easier because each stage can be inspected individually,\n",
    "- training data for downstream models can be built from these JSON files without requiring\n",
    "  access to the original video or audio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0991406",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "This section defines the configuration used throughout the notebook:\n",
    "\n",
    "- File system locations under `data/`.\n",
    "- Video specific parameters (input path, frame sampling interval).\n",
    "- ASR configuration (model size, device, quantisation type).\n",
    "- OCR configuration (how many frames are actually processed).\n",
    "- Summarisation configuration (model, device and text length limits).\n",
    "\n",
    "The values below are reasonable defaults for a first run and can be adapted per project.\n",
    "In particular:\n",
    "\n",
    "- `video_path` must be updated so that it points to a valid local file.\n",
    "- When no GPU is available, `asr_device` should be set to `\"cpu\"` and `summary_device`\n",
    "  to `-1`. This will be slower, but the pipeline remains fully functional.\n",
    "- The summarisation limits (`summary_max_chars`, `summary_max_length`, `summary_min_length`)\n",
    "  control the trade off between execution time and level of detail in the final summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497787d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video file:      C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\input.mp4\n",
      "Frames dir:      C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\frames\n",
      "Audio path:      C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\audio.wav\n",
      "Transcript path: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\transcript.json\n",
      "OCR output:      C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\ocr.json\n",
      "Segments path:   C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\segments.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Root data directories\n",
    "data_dir = os.path.join(project_root, \"data\")\n",
    "interim_dir = os.path.join(data_dir, \"interim\") # heavy intermediate files (frames, JSON)\n",
    "\n",
    "# Input video to analyse (update this to your own file)\n",
    "video_path = os.path.join(data_dir, \"input.mp4\")\n",
    "\n",
    "# Intermediate artefacts\n",
    "frame_dir = os.path.join(interim_dir, \"frames\")\n",
    "audio_path = os.path.join(interim_dir, \"audio.wav\")\n",
    "transcript_path = os.path.join(interim_dir, \"transcript.json\")\n",
    "ocr_output_path = os.path.join(interim_dir, \"ocr.json\")\n",
    "segments_path = os.path.join(interim_dir, \"segments.json\")\n",
    "\n",
    "# Frame extraction and OCR\n",
    "frame_interval_seconds = 3  # distance between extracted frames in seconds\n",
    "ocr_frame_stride = 2        # process every Nth frame for OCR in order to reduce cost\n",
    "\n",
    "# ASR configuration for faster-whisper\n",
    "asr_model_size = \"small\"       # for example: \"tiny\", \"base\", \"small\", \"medium\", \"large-v2\"\n",
    "asr_device = \"cuda\"            # \"cuda\" or \"cpu\"\n",
    "asr_compute_type = \"int8\"      # quantisation used by faster-whisper, see its documentation\n",
    "\n",
    "# Summarisation configuration\n",
    "summary_model_name = \"facebook/bart-large-cnn\"\n",
    "summary_device = 0            # GPU index, or -1 for CPU\n",
    "summary_max_chars = 3000      # max number of characters per chunk of transcript\n",
    "summary_max_length = 500      # max number of tokens in the summary of one chunk\n",
    "summary_min_length = 40       # min number of tokens in the summary of one chunk\n",
    "\n",
    "print(\"Video file:     \", video_path)\n",
    "print(\"Frames dir:     \", frame_dir)\n",
    "print(\"Audio path:     \", audio_path)\n",
    "print(\"Transcript path:\", transcript_path)\n",
    "print(\"OCR output:     \", ocr_output_path)\n",
    "print(\"Segments path:  \", segments_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ef14f",
   "metadata": {},
   "source": [
    "## Prepare directories\n",
    "\n",
    "The following cell creates the expected directory structure if it does not already exist.\n",
    "\n",
    "The convention used here is:\n",
    "\n",
    "- all input artefacts are stored in `data/raw/`,\n",
    "- all heavy intermediate artefacts are stored in `data/interim/`,\n",
    "- additional cleaned datasets, if any, can be stored in `data/processed/` by other notebooks.\n",
    "\n",
    "The frame directory is created under `data/interim/frames/`. It acts both as cache for already\n",
    "extracted frames and as the input source for the OCR stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f654b963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure prepared.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for d in [data_dir, interim_dir, frame_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"Directory structure prepared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f063423",
   "metadata": {},
   "source": [
    "## 1. Inspect input video\n",
    "\n",
    "The `inspect_video` helper uses `moviepy` to open the input file and prints basic information,\n",
    "such as:\n",
    "\n",
    "- duration (seconds),\n",
    "- resolution,\n",
    "- frame rate,\n",
    "- file size.\n",
    "\n",
    "This serves mainly as a sanity check that the file is readable and helps to choose reasonable\n",
    "values for frame sampling (for example, how many frames will be produced with the current\n",
    "`frame_interval_seconds` setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2bb3a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found video file: input.mp4\n",
      "Size: 93.71 MB\n",
      "Duration: 1075.39 seconds (17.92 minutes)\n",
      "Frame rate (fps): 60.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    inspect_video(str(video_path))\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce3239",
   "metadata": {},
   "source": [
    "## 2. Extract audio and frames\n",
    "\n",
    "The ingest stage performs two operations:\n",
    "\n",
    "1. **Audio extraction**\n",
    "\n",
    "   The `extract_audio` function opens the input video with `moviepy`, extracts the audio track\n",
    "   and writes it to a `wav` file. The code is careful to avoid recomputation when the file\n",
    "   already exists on disk.\n",
    "\n",
    "   This audio file is the only input required by the ASR stage.\n",
    "\n",
    "2. **Frame sampling**\n",
    "\n",
    "   The `extract_frames` function iterates over the video at a fixed temporal interval\n",
    "   (`frame_interval_seconds`) and saves each sampled frame to the `frame_dir` folder as a JPEG\n",
    "   image. File names encode the time index (for example `frame_000120.jpg` for a frame taken\n",
    "   at t ≈ 120 s).\n",
    "\n",
    "   The function also exposes a light `preview` mode that prints how many frames were written\n",
    "   and lists the first few frame names. This is useful to verify that the sampling frequency\n",
    "   is appropriate for the type of video being processed.\n",
    "\n",
    "Both steps are idempotent: running the cell multiple times is safe and will not corrupt\n",
    "the existing artefacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99cdd58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio track ...\n",
      "MoviePy - Writing audio in C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Saved audio to: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\audio.wav\n",
      "Planned number of frames: 359\n",
      "Saving frames to: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 359/359 [00:59<00:00,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio present: True\n",
      "Number of frame files: 359\n",
      "First few frame files:\n",
      " - frame_00000.jpg\n",
      " - frame_00001.jpg\n",
      " - frame_00002.jpg\n",
      " - frame_00003.jpg\n",
      " - frame_00004.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_audio(str(video_path), str(audio_path))\n",
    "\n",
    "extract_frames(\n",
    "    video_path=str(video_path),\n",
    "    frame_dir=str(frame_dir),\n",
    "    interval_seconds=frame_interval_seconds,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8a57a",
   "metadata": {},
   "source": [
    "## 3. Automatic speech recognition (ASR)\n",
    "\n",
    "The ASR stage converts raw audio into a list of time stamped transcript segments. The\n",
    "implementation in `src/asr.py` is based on `faster-whisper`, which provides a fast and memory\n",
    "efficient inference wrapper around OpenAI Whisper models.\n",
    "\n",
    "Main characteristics of `run_asr`:\n",
    "\n",
    "- Loads the specified Whisper checkpoint (for example `\"small\"` or `\"medium\"`).\n",
    "- Runs inference on the `audio_path` in streaming mode and collects the segments emitted by\n",
    "  Whisper. Each segment has a start time, an end time and a text field.\n",
    "- Wraps each raw segment into a `TranscriptSegment` dataclass instance.\n",
    "- Writes the full list of segments to `transcript_path` as JSON, using the helper\n",
    "  `transcript_to_jsonable` defined in `models.py`.\n",
    "- Prints the number of segments produced and the location of the JSON file.\n",
    "\n",
    "The choice of `asr_model_size`, `asr_device` and `asr_compute_type` controls the trade off\n",
    "between speed, resource usage and recognition quality.\n",
    "\n",
    "The helper `preview_transcript` is provided as a light inspection tool. It prints the first `n`\n",
    "segments with human readable timestamps and is designed to be called from notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5c4663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading faster-whisper model …\n",
      "Transcribing audio: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\audio.wav\n",
      "Saved transcript to: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\transcript.json\n",
      "Number of transcript segments: 265\n",
      "Total segments: 265\n",
      "[0.00 -> 6.04] This is a 3.\n",
      "[6.04 -> 11.52] It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain\n",
      "[11.52 -> 14.34] has no trouble recognizing it as a 3.\n",
      "[14.34 -> 18.52] And I want you to take a moment to appreciate how crazy it is that brains can do this so\n",
      "[18.52 -> 19.52] effortlessly.\n"
     ]
    }
   ],
   "source": [
    "transcript_segments = run_asr(\n",
    "    audio_path=str(audio_path),\n",
    "    transcript_path=str(transcript_path),\n",
    "    model_size=asr_model_size,\n",
    "    device=asr_device,\n",
    "    compute_type=asr_compute_type,\n",
    ")\n",
    "\n",
    "preview_transcript(transcript_segments, n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5414823",
   "metadata": {},
   "source": [
    "## 4. Optical character recognition (OCR) on frames\n",
    "\n",
    "The OCR stage recovers slide text from the video frames sampled during ingestion. It operates\n",
    "on the images stored in `frame_dir` and proceeds as follows:\n",
    "\n",
    "- Lists all frame files that match the expected naming pattern (for example\n",
    "  `frame_000120.jpg`). Each file name encodes the approximate time at which the frame was\n",
    "  captured.\n",
    "- Iterates over the frames in chronological order and processes only every\n",
    "  `ocr_frame_stride`-th frame in order to keep the total OCR cost manageable.\n",
    "- For each selected frame:\n",
    "  - Opens the file with `PIL.Image`.\n",
    "  - Runs OCR using `pytesseract.image_to_string` with a configuration tuned for slide text\n",
    "    (single uniform block, no page segmentation into multiple columns).\n",
    "  - Constructs an `OCRRecord` object with the estimated timestamp (derived from the frame\n",
    "    index and `frame_interval_seconds`), the frame file name and the recognised text.\n",
    "\n",
    "The list of `OCRRecord` instances is serialised to `ocr_output_path` as JSON using the helper\n",
    "`ocr_to_jsonable`. The `preview_ocr` function prints the first records in a readable form\n",
    "including the approximate time and the corresponding frame file.\n",
    "\n",
    "The downstream alignment step only relies on the timestamps and the extracted text. If OCR\n",
    "quality is low for a particular video, the sampling frequency and OCR configuration can be\n",
    "tuned independently of the rest of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb92053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running OCR on sampled frames …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 359/359 [00:49<00:00,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved OCR output for 180 frames to: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\ocr.json\n",
      "OCR records: 180\n",
      "[t ~ 0.0s] frame=frame_00000.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 6.0s] frame=frame_00002.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 12.0s] frame=frame_00004.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 18.0s] frame=frame_00006.jpg\n",
      "\n",
      "----------------------------------------\n",
      "[t ~ 24.0s] frame=frame_00008.jpg\n",
      "\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ocr_records = run_ocr_on_frames(\n",
    "    frame_dir=str(frame_dir),\n",
    "    ocr_output_path=str(ocr_output_path),\n",
    "    frame_interval_seconds=frame_interval_seconds,\n",
    "    ocr_frame_stride=ocr_frame_stride,\n",
    ")\n",
    "\n",
    "preview_ocr(ocr_records, n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8ad02",
   "metadata": {},
   "source": [
    "## 5. Align transcript and slide text\n",
    "\n",
    "The goal of the alignment stage is to attach the most relevant slide text to each speech\n",
    "segment. The implementation in `src/align.py` performs a simple, but effective, temporal\n",
    "nearest neighbour matching.\n",
    "\n",
    "High level algorithm of `align_transcript_and_ocr`:\n",
    "\n",
    "1. Precompute a list of OCR times (one value per `OCRRecord`).\n",
    "2. For each `TranscriptSegment` in chronological order:\n",
    "   - locate the index of the nearest OCR time using a binary search over the sorted list\n",
    "     (this is implemented in the helper `_find_nearest_ocr_index`),\n",
    "   - copy the corresponding slide text and frame information into a new `Segment` instance.\n",
    "3. Apply a light post-processing step that merges very short segments and removes borderline\n",
    "   duplicates in order to obtain smoother segments.\n",
    "\n",
    "Each resulting `Segment` therefore contains:\n",
    "\n",
    "- the start and end time of the speech segment,\n",
    "- the speech text as produced by ASR,\n",
    "- the slide text and corresponding slide timestamp,\n",
    "- the name of the frame from which the slide text was extracted.\n",
    "\n",
    "The list of segments is saved to `segments_path` as JSON to make it easy to inspect or reuse\n",
    "in downstream tasks.\n",
    "\n",
    "The helper `preview_segments` prints the first `n` aligned segments, showing both the speech\n",
    "and the associated slide text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce27eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aligned multimodal segments to: C:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data\\interim\\segments.json\n",
      "Total segments: 265\n",
      "Aligned segments: 265\n",
      "[0.00 -> 6.04] (slide at 6.0) speech='This is a 3....'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[6.04 -> 11.52] (slide at 6.0) speech='It's sloppily written and rendered at an extremely low resol...'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[11.52 -> 14.34] (slide at 12.0) speech='has no trouble recognizing it as a 3....'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[14.34 -> 18.52] (slide at 18.0) speech='And I want you to take a moment to appreciate how crazy it i...'\n",
      "Slide text: \n",
      "----------------------------------------\n",
      "[18.52 -> 19.52] (slide at 18.0) speech='effortlessly....'\n",
      "Slide text: \n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "segments = align_transcript_and_ocr(\n",
    "    transcript_segments=transcript_segments,\n",
    "    ocr_records=ocr_records,\n",
    "    segments_path=str(segments_path),\n",
    ")\n",
    "\n",
    "preview_segments(segments, n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f56e21",
   "metadata": {},
   "source": [
    "## 6. Global abstractive summary\n",
    "\n",
    "The final stage compresses the aligned transcript into a short textual summary. The function\n",
    "`summarise_segments` in `src/summarise.py` uses the following strategy:\n",
    "\n",
    "1. Concatenate the `speech` fields of all segments into a single long transcript string.\n",
    "2. Split this string into chunks using `_chunk_text`, which respects sentence boundaries\n",
    "   when possible and ensures that each chunk does not exceed `max_chunk_chars` characters.\n",
    "3. Instantiate a Hugging Face `pipeline` with a summarisation model (by default\n",
    "   `\"facebook/bart-large-cnn\"`) on the requested device (GPU index or CPU).\n",
    "4. For each chunk:\n",
    "   - call the summariser with `max_length` and `min_length` constraints,\n",
    "   - collect the resulting partial summary text.\n",
    "5. Concatenate all chunk-level summaries into one global summary string.\n",
    "\n",
    "The chunking mechanism is important for two reasons:\n",
    "\n",
    "- it allows the use of standard encoder-decoder models with relatively short maximum input\n",
    "  lengths on arbitrarily long transcripts,\n",
    "- it constrains the worst case memory usage during generation.\n",
    "\n",
    "`summarise_segments` prints the global summary to standard output and returns it as a string\n",
    "for further processing in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b53f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transcript length (characters): 17906\n",
      "Number of chunks for summarisation: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising chunk 1/7 …\n",
      "Summarising chunk 2/7 …\n",
      "Summarising chunk 3/7 …\n",
      "Summarising chunk 4/7 …\n",
      "Summarising chunk 5/7 …\n",
      "Summarising chunk 6/7 …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 500, but your input_length is only 57. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising chunk 7/7 …\n",
      "\n",
      "=== GLOBAL SUMMARY ===\n",
      "\n",
      "In this video, we look at how a neural network can learn to recognize handwritten digits. In the next, we'll look at the structure component of that. At the end of the two videos, I want to point you to a couple of good resources where you can learn more.\n",
      "The network starts with a bunch of neurons corresponding to each of the 28 times 28 pixels of the input image, which is 784 neurons in total. The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit.\n",
      "The goal is to have some mechanism that could conceivably combine pixels into edges, or edges into patterns, or patterns into digits. In a perfect world, we might hope that each neuron in the second to last layer of the network corresponds with one of these subcomponents.\n",
      "The question at hand is, what parameters should the network have? What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern, or any other pixel pattern? Well, what we'll do is assign a weight to each one of the connections between our neurons.\n",
      "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights and 16 biases. The connections between the other layers also have a bunch of weights and biasesassociated with them. This network has almost exactly 13,000 total weights and bias.\n",
      "The entire network is just a function. sigmoid function to each specific component of the resulting vector inside. Once you write down this weight matrix and these vectors as their own symbols, you can communicate the full transition of activations from one layer to the next.\n",
      "Using sigmoids didn't help training or it was very difficult to train at some point, and people just tried relu. simplification. And it happened to work very well for these incredibly deep neuralnetworks.\n"
     ]
    }
   ],
   "source": [
    "summary = summarise_segments(\n",
    "    segments=segments,\n",
    "    model_name=summary_model_name,\n",
    "    device=summary_device,\n",
    "    max_chunk_chars=summary_max_chars,\n",
    "    max_length=summary_max_length,\n",
    "    min_length=summary_min_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0890d90",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This introduction notebook demonstrates the complete multimodal pipeline, starting from a raw\n",
    "video file and producing:\n",
    "\n",
    "- an audio transcript derived from the video soundtrack,\n",
    "- slide text extracted from sampled frames,\n",
    "- aligned multimodal segments that combine both modalities,\n",
    "- an abstractive summary suitable for fast review or indexing.\n",
    "\n",
    "All heavy computation is delegated to the reusable modules under `src/`, while the notebook\n",
    "focuses on configuration and orchestration. This separation permits straightforward extension\n",
    "of the project with additional experiments, visualisations or alternative models in dedicated\n",
    "notebooks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (multimodal)",
   "language": "python",
   "name": "multimodal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
