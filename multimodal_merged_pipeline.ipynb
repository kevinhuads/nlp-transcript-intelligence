{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal NLP: Ingestion + Core Pipeline\n",
        "\n",
        "This notebook combines the previous **video ingestion** and **core processing** logic into a single end‑to‑end pipeline:\n",
        "\n",
        "1. Inspect a local input video `input.mp4`.\n",
        "2. Extract audio as `audio.wav`.\n",
        "3. Extract frames at a fixed interval.\n",
        "4. Run ASR with `faster-whisper` and save a time coded transcript.\n",
        "5. Run OCR on sampled frames.\n",
        "6. Align ASR segments with nearby OCR frames and export `segments.json`.\n",
        "7. Build a global abstractive summary of the transcript.\n",
        "\n",
        "All artefacts are written under `data_example_video/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory: c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\n",
            "Video path: c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\input.mp4\n",
            "Output directory: c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data_example_video\n",
            "Frame directory: c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data_example_video\\frames\n"
          ]
        }
      ],
      "source": [
        "# Optional installations (uncomment in a fresh environment)\n",
        "# !pip install moviepy tqdm faster-whisper pytesseract pillow transformers sentencepiece\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from moviepy import VideoFileClip\n",
        "from tqdm import tqdm\n",
        "\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Configure Tesseract path if needed (example for Windows)\n",
        "# pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
        "\n",
        "DATA_DIR = Path.cwd()\n",
        "VIDEO_PATH = DATA_DIR / \"input.mp4\"\n",
        "VIDEO_DIR = DATA_DIR / \"data_example_video\"\n",
        "AUDIO_PATH = VIDEO_DIR / \"audio.wav\"\n",
        "FRAME_DIR = VIDEO_DIR / \"frames\"\n",
        "\n",
        "VIDEO_DIR.mkdir(exist_ok=True, parents=True)\n",
        "FRAME_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(f\"Working directory: {DATA_DIR}\")\n",
        "print(f\"Video path: {VIDEO_PATH}\")\n",
        "print(f\"Output directory: {VIDEO_DIR}\")\n",
        "print(f\"Frame directory: {FRAME_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Video ingestion\n",
        "### 1.1 Basic checks on `input.mp4`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found video file: input.mp4\n",
            "Size: 93.71 MB\n",
            "Duration: 1075.39 seconds (17.92 minutes)\n",
            "Frame rate (fps): 60.0\n"
          ]
        }
      ],
      "source": [
        "if not VIDEO_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find {VIDEO_PATH}. Please place 'input.mp4' next to this notebook.\"\n",
        "    )\n",
        "\n",
        "file_size_mb = VIDEO_PATH.stat().st_size / (1024 * 1024)\n",
        "print(f\"Found video file: {VIDEO_PATH.name}\")\n",
        "print(f\"Size: {file_size_mb:.2f} MB\")\n",
        "\n",
        "clip = VideoFileClip(str(VIDEO_PATH))\n",
        "duration = clip.duration  # seconds\n",
        "fps = clip.fps\n",
        "\n",
        "print(f\"Duration: {duration:.2f} seconds ({duration/60:.2f} minutes)\")\n",
        "print(f\"Frame rate (fps): {fps}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Audio extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audio file already exists: c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data_example_video\\audio.wav\n"
          ]
        }
      ],
      "source": [
        "if AUDIO_PATH.exists():\n",
        "    print(f\"Audio file already exists: {AUDIO_PATH}\")\n",
        "else:\n",
        "    print(\"Extracting audio track ...\")\n",
        "    clip.audio.write_audiofile(str(AUDIO_PATH))\n",
        "    print(f\"Saved audio to: {AUDIO_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Frame extraction\n",
        "Extract JPEG frames at a fixed interval (in seconds).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Planned number of frames: 359\n",
            "Saving frames to: c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data_example_video\\frames\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 359/359 [00:56<00:00,  6.31it/s]\n"
          ]
        }
      ],
      "source": [
        "FRAME_INTERVAL_SECONDS = 3  # adjust as needed\n",
        "\n",
        "n_frames = int(duration // FRAME_INTERVAL_SECONDS) + 1\n",
        "print(f\"Planned number of frames: {n_frames}\")\n",
        "print(f\"Saving frames to: {FRAME_DIR}\")\n",
        "\n",
        "for i, t in enumerate(tqdm(range(0, int(duration) + 1, FRAME_INTERVAL_SECONDS))):\n",
        "    frame_time = min(t, duration)\n",
        "    frame = clip.get_frame(frame_time)\n",
        "    frame_path = FRAME_DIR / f\"frame_{i:05d}.jpg\"\n",
        "    if not frame_path.exists():\n",
        "        img = Image.fromarray(frame)\n",
        "        img.save(frame_path, format=\"JPEG\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Sanity check of extracted artefacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audio present: True\n",
            "Number of frame files: 359\n",
            "First few frame files:\n",
            " - frame_00000.jpg\n",
            " - frame_00001.jpg\n",
            " - frame_00002.jpg\n",
            " - frame_00003.jpg\n",
            " - frame_00004.jpg\n"
          ]
        }
      ],
      "source": [
        "audio_exists = AUDIO_PATH.exists()\n",
        "frame_files = sorted(FRAME_DIR.glob(\"frame_*.jpg\"))\n",
        "n_extracted_frames = len(frame_files)\n",
        "\n",
        "print(f\"Audio present: {audio_exists}\")\n",
        "print(f\"Number of frame files: {n_extracted_frames}\")\n",
        "print(\"First few frame files:\")\n",
        "for f in frame_files[:5]:\n",
        "    print(\" -\", f.name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Core processing: ASR, OCR, alignment, summary\n",
        "### 2.1 Checks on audio and frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found audio: c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data_example_video\\audio.wav\n",
            "Number of frames found: 359\n",
            "First few frames:\n",
            " - frame_00000.jpg\n",
            " - frame_00001.jpg\n",
            " - frame_00002.jpg\n",
            " - frame_00003.jpg\n",
            " - frame_00004.jpg\n"
          ]
        }
      ],
      "source": [
        "if not AUDIO_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Expected audio file at {AUDIO_PATH}, but it does not exist.\")\n",
        "\n",
        "if not FRAME_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Expected frame directory at {FRAME_DIR}, but it does not exist.\")\n",
        "\n",
        "frame_files = sorted(FRAME_DIR.glob(\"frame_*.jpg\"))\n",
        "print(f\"Found audio: {AUDIO_PATH}\")\n",
        "print(f\"Number of frames found: {len(frame_files)}\")\n",
        "print(\"First few frames:\")\n",
        "for f in frame_files[:5]:\n",
        "    print(\" -\", f.name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 ASR with `faster-whisper`\n",
        "Produces `transcript_segments.json` with `start`, `end`, and `text` fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcript file already exists: c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data_example_video\\transcript_segments.json\n",
            "Loaded 373 transcript segments.\n"
          ]
        }
      ],
      "source": [
        "TRANSCRIPT_PATH = VIDEO_DIR / \"transcript_segments.json\"\n",
        "\n",
        "if TRANSCRIPT_PATH.exists():\n",
        "    print(f\"Transcript file already exists: {TRANSCRIPT_PATH}\")\n",
        "    with open(TRANSCRIPT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        transcript_segments = json.load(f)\n",
        "    print(f\"Loaded {len(transcript_segments)} transcript segments.\")\n",
        "else:\n",
        "    print(\"Loading faster-whisper model ...\")\n",
        "    model = WhisperModel(\"small\", device=\"cuda\", compute_type=\"int8\")\n",
        "\n",
        "    print(f\"Transcribing audio: {AUDIO_PATH}\")\n",
        "    segments, info = model.transcribe(str(AUDIO_PATH), beam_size=5)\n",
        "\n",
        "    transcript_segments = []\n",
        "    for seg in segments:\n",
        "        transcript_segments.append(\n",
        "            {\n",
        "                \"start\": float(seg.start),\n",
        "                \"end\": float(seg.end),\n",
        "                \"text\": seg.text.strip(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    with open(TRANSCRIPT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(transcript_segments, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved transcript to: {TRANSCRIPT_PATH}\")\n",
        "    print(f\"Number of transcript segments: {len(transcript_segments)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total segments: 373\n",
            "[0.00 -> 6.04] This is a 3.\n",
            "[6.04 -> 11.52] It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain\n",
            "[11.52 -> 14.34] has no trouble recognizing it as a 3.\n",
            "[14.34 -> 18.52] And I want you to take a moment to appreciate how crazy it is that brains can do this so\n",
            "[18.52 -> 19.52] effortlessly.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total segments: {len(transcript_segments)}\")\n",
        "for seg in transcript_segments[:5]:\n",
        "    print(f\"[{seg['start']:.2f} -> {seg['end']:.2f}] {seg['text']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 OCR on sampled frames\n",
        "Runs Tesseract OCR on a subset of frames and stores the result in `ocr_frames.json`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OCR output already exists: c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data_example_video\\ocr_frames.json\n",
            "Loaded OCR text for 180 frames.\n"
          ]
        }
      ],
      "source": [
        "OCR_OUTPUT_PATH = VIDEO_DIR / \"ocr_frames.json\"\n",
        "\n",
        "# Must match the interval used when extracting frames\n",
        "FRAME_INTERVAL_SECONDS = 3\n",
        "\n",
        "# Sample every nth frame for OCR\n",
        "OCR_FRAME_STRIDE = 2\n",
        "\n",
        "if OCR_OUTPUT_PATH.exists():\n",
        "    print(f\"OCR output already exists: {OCR_OUTPUT_PATH}\")\n",
        "    with open(OCR_OUTPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        ocr_records = json.load(f)\n",
        "    print(f\"Loaded OCR text for {len(ocr_records)} frames.\")\n",
        "else:\n",
        "    ocr_records = []\n",
        "    print(\"Running OCR on sampled frames ...\")\n",
        "\n",
        "    frame_files = sorted(FRAME_DIR.glob(\"frame_*.jpg\"))\n",
        "\n",
        "    for idx, frame_path in enumerate(tqdm(frame_files)):\n",
        "        if idx % OCR_FRAME_STRIDE != 0:\n",
        "            continue\n",
        "\n",
        "        approx_time = idx * FRAME_INTERVAL_SECONDS\n",
        "\n",
        "        with Image.open(frame_path) as img:\n",
        "            text = pytesseract.image_to_string(img)\n",
        "\n",
        "        ocr_records.append(\n",
        "            {\n",
        "                \"time\": float(approx_time),\n",
        "                \"frame\": frame_path.name,\n",
        "                \"text\": text.strip(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    with open(OCR_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(ocr_records, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved OCR output for {len(ocr_records)} frames to: {OCR_OUTPUT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OCR records: 180\n",
            "[t ~ 0.0s] frame=frame_00000.jpg\n",
            "\n",
            "----------------------------------------\n",
            "[t ~ 6.0s] frame=frame_00002.jpg\n",
            "\n",
            "----------------------------------------\n",
            "[t ~ 12.0s] frame=frame_00004.jpg\n",
            "\n",
            "----------------------------------------\n",
            "[t ~ 18.0s] frame=frame_00006.jpg\n",
            "\n",
            "----------------------------------------\n",
            "[t ~ 24.0s] frame=frame_00008.jpg\n",
            "\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(f\"OCR records: {len(ocr_records)}\")\n",
        "for rec in ocr_records[:5]:\n",
        "    print(f\"[t ~ {rec['time']:.1f}s] frame={rec['frame']}\")\n",
        "    print(rec['text'])\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Temporal alignment of ASR and OCR\n",
        "Aligns each ASR segment with the nearest OCR time stamp and exports `segments.json`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved aligned multimodal segments to: c:\\Users\\kevin\\OneDrive\\Documents\\Work\\Python\\NLP-Videos\\data_example_video\\segments.json\n",
            "Total segments: 373\n"
          ]
        }
      ],
      "source": [
        "SEGMENTS_PATH = VIDEO_DIR / \"segments.json\"\n",
        "\n",
        "ocr_times = [rec[\"time\"] for rec in ocr_records]\n",
        "\n",
        "def find_nearest_ocr_index(target_time: float) -> int:\n",
        "    if not ocr_times:\n",
        "        return -1\n",
        "    best_idx = 0\n",
        "    best_diff = abs(ocr_times[0] - target_time)\n",
        "    for i, t in enumerate(ocr_times[1:], start=1):\n",
        "        diff = abs(t - target_time)\n",
        "        if diff < best_diff:\n",
        "            best_diff = diff\n",
        "            best_idx = i\n",
        "    return best_idx\n",
        "\n",
        "segments_merged = []\n",
        "\n",
        "for seg in transcript_segments:\n",
        "    mid = 0.5 * (seg[\"start\"] + seg[\"end\"])\n",
        "    ocr_idx = find_nearest_ocr_index(mid)\n",
        "\n",
        "    if ocr_idx == -1:\n",
        "        ocr_text = \"\"\n",
        "        ocr_time = None\n",
        "        ocr_frame = None\n",
        "    else:\n",
        "        ocr_rec = ocr_records[ocr_idx]\n",
        "        ocr_text = ocr_rec[\"text\"]\n",
        "        ocr_time = ocr_rec[\"time\"]\n",
        "        ocr_frame = ocr_rec[\"frame\"]\n",
        "\n",
        "    segments_merged.append(\n",
        "        {\n",
        "            \"start\": seg[\"start\"],\n",
        "            \"end\": seg[\"end\"],\n",
        "            \"mid\": mid,\n",
        "            \"speech\": seg[\"text\"],\n",
        "            \"slide_text\": ocr_text,\n",
        "            \"slide_time\": ocr_time,\n",
        "            \"slide_frame\": ocr_frame,\n",
        "        }\n",
        "    )\n",
        "\n",
        "with open(SEGMENTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(segments_merged, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Saved aligned multimodal segments to: {SEGMENTS_PATH}\")\n",
        "print(f\"Total segments: {len(segments_merged)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aligned segments: 373\n",
            "[0.00 -> 6.04] (slide at 6.0) speech='This is a 3....'\n",
            "Slide text: \n",
            "----------------------------------------\n",
            "[6.04 -> 11.52] (slide at 6.0) speech='It's sloppily written and rendered at an extremely low resol...'\n",
            "Slide text: \n",
            "----------------------------------------\n",
            "[11.52 -> 14.34] (slide at 12.0) speech='has no trouble recognizing it as a 3....'\n",
            "Slide text: \n",
            "----------------------------------------\n",
            "[14.34 -> 18.52] (slide at 18.0) speech='And I want you to take a moment to appreciate how crazy it i...'\n",
            "Slide text: \n",
            "----------------------------------------\n",
            "[18.52 -> 19.52] (slide at 18.0) speech='effortlessly....'\n",
            "Slide text: \n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(f\"Aligned segments: {len(segments_merged)}\")\n",
        "for seg in segments_merged[:5]:\n",
        "    print(\n",
        "        f\"[{seg['start']:.2f} -> {seg['end']:.2f}] (slide at {seg['slide_time']}) speech='{seg['speech'][:60]}...'\"\n",
        "    )\n",
        "    print(f\"Slide text: {seg['slide_text'][:200]}\")\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Global abstractive summary\n",
        "Concatenate all speech segments and summarise them in chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total transcript length (characters): 17934\n",
            "This is a 3.\n",
            "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain\n",
            "has no trouble recognizing it as a 3.\n",
            "And I want you to take a moment to appreciate how crazy it is that brains can do this so\n",
            "effortlessly.\n",
            "I mean, this, this, and this are also recognizable as 3s, even though the specific values\n",
            "of each pixel is very different from one image to the next.\n",
            "The particular light-sensitive cells in your eye that are firing when you see this 3 are\n",
            "very different from the ones firing when you see this 3.\n",
            "But something in that crazy smart visual cortex of yours resolves these as representing the\n",
            "same idea, while at the same time recognizing other images as their own distinct ideas.\n",
            "But if I told you, hey, sit down and write for me a program that takes in a grid of\n",
            "28x28 pixels like this, and outputs a single number between 0 and 10, telling you what it\n",
            "thinks the digit is, well, the task goes from comically trivial to dauntingly difficult.\n",
            "Unless yo...\n"
          ]
        }
      ],
      "source": [
        "full_transcript_text = \"\\n\".join(seg[\"speech\"] for seg in segments_merged)\n",
        "\n",
        "print(f\"Total transcript length (characters): {len(full_transcript_text)}\")\n",
        "preview = full_transcript_text[:1000]\n",
        "if len(full_transcript_text) > 1000:\n",
        "    preview += \"...\"\n",
        "print(preview)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Your max_length is set to 5000, but your input_length is only 675. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=337)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks for summarisation: 7\n",
            "Summarising chunk 1/7 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 5000, but your input_length is only 671. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=335)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarising chunk 2/7 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 5000, but your input_length is only 678. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=339)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarising chunk 3/7 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 5000, but your input_length is only 682. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=341)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarising chunk 4/7 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 5000, but your input_length is only 681. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=340)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarising chunk 5/7 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 5000, but your input_length is only 700. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=350)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarising chunk 6/7 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 5000, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarising chunk 7/7 ...\n",
            "\n",
            "=== GLOBAL SUMMARY ===\n",
            "\n",
            "In this video, we look at how a neural network can learn to recognize handwritten digits. In the next, we'll look at the structure of the network, and how it's connected to other neurons.\n",
            "The network I'm showing here has already been trained to recognize digits. The way the network operates, activations in one layer determine the activations of the next layer. The brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents.\n",
            "In a perfect world, we might hope that each neuron in the second to last layer of the network corresponds with one of these subcomponents. That way, going from the third layer to the last one just requires learning which combination of subcomp components corresponds to which digits.\n",
            "The question at hand is what parameters should the network have? What dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern or any other pixel pattern or the pattern that several edges can make a loop.\n",
            "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights                along with 16 biases. The connections between the other layers                also have a bunch of weights and biasesassociated with them.\n",
            "The entire network is just a function. One that takes in 784 numbers as an input and spits out 10 numbers as a output. It's an absurdly complicated function, but in a way, it's kind of reassuring that it looks complicated.\n",
            "Alicia: It was very difficult to train at some point and people just tried relu                and it happened to work very wellfor these incredibly deep neural networks. All right, thank you, Alicia.\n"
          ]
        }
      ],
      "source": [
        "summariser = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\",\n",
        "    device=0,  # first GPU; use -1 for CPU\n",
        ")\n",
        "\n",
        "max_chunk_chars = 3000\n",
        "max_length = 5000\n",
        "\n",
        "def chunk_text(text, max_chars):\n",
        "    chunks = []\n",
        "    current = []\n",
        "    current_len = 0\n",
        "    for line in text.split(\"\\n\"):\n",
        "        line_len = len(line) + 1\n",
        "        if current_len + line_len > max_chars and current:\n",
        "            chunks.append(\"\\n\".join(current))\n",
        "            current = [line]\n",
        "            current_len = line_len\n",
        "        else:\n",
        "            current.append(line)\n",
        "            current_len += line_len\n",
        "    if current:\n",
        "        chunks.append(\"\\n\".join(current))\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(full_transcript_text, max_chunk_chars)\n",
        "print(f\"Number of chunks for summarisation: {len(chunks)}\")\n",
        "\n",
        "summaries = []\n",
        "for idx, ch in enumerate(chunks):\n",
        "    print(f\"Summarising chunk {idx + 1}/{len(chunks)} ...\")\n",
        "    out = summariser(ch, max_length=max_length, min_length=40, do_sample=False)\n",
        "    summaries.append(out[0][\"summary_text\"])\n",
        "\n",
        "global_summary = \"\\n\".join(summaries)\n",
        "\n",
        "print(\"\\n=== GLOBAL SUMMARY ===\\n\")\n",
        "print(global_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generated artefacts\n",
        "\n",
        "Under `data_example_video/`:\n",
        "\n",
        "- `audio.wav`: extracted audio track.\n",
        "- `frames/frame_XXXXX.jpg`: frames every `FRAME_INTERVAL_SECONDS` seconds.\n",
        "- `transcript_segments.json`: ASR segments with time codes.\n",
        "- `ocr_frames.json`: OCR results on sampled frames.\n",
        "- `segments.json`: aligned multimodal segments (speech + slide text).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "625263aa",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "98d8ead4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# __init__.py\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# align.py\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "import json\n",
            "import os\n",
            "from typing import List\n",
            "\n",
            "from .models import TranscriptSegment, OCRRecord, Segment, segments_to_jsonable\n",
            "\n",
            "\n",
            "def _precompute_ocr_times(ocr_records: List[OCRRecord]) -> List[float]:\n",
            "    return [rec.time for rec in ocr_records]\n",
            "\n",
            "\n",
            "def _find_nearest_ocr_index(\n",
            "    ocr_times: List[float],\n",
            "    target_time: float,\n",
            ") -> int:\n",
            "    if not ocr_times:\n",
            "        return -1\n",
            "    best_idx = 0\n",
            "    best_diff = abs(ocr_times[0] - target_time)\n",
            "    for i, t in enumerate(ocr_times[1:], start=1):\n",
            "        diff = abs(t - target_time)\n",
            "        if diff < best_diff:\n",
            "            best_diff = diff\n",
            "            best_idx = i\n",
            "    return best_idx\n",
            "\n",
            "\n",
            "def align_transcript_and_ocr(\n",
            "    transcript_segments: List[TranscriptSegment],\n",
            "    ocr_records: List[OCRRecord],\n",
            "    segments_path: str,\n",
            ") -> List[Segment]:\n",
            "    segments_dir = os.path.dirname(segments_path)\n",
            "    if segments_dir:\n",
            "        os.makedirs(segments_dir, exist_ok=True)\n",
            "\n",
            "    ocr_times = _precompute_ocr_times(ocr_records)\n",
            "    segments_merged: List[Segment] = []\n",
            "\n",
            "    for seg in transcript_segments:\n",
            "        mid = 0.5 * (seg.start + seg.end)\n",
            "        ocr_idx = _find_nearest_ocr_index(ocr_times, mid)\n",
            "\n",
            "        if ocr_idx == -1:\n",
            "            ocr_text = \"\"\n",
            "            ocr_time = None\n",
            "            ocr_frame = None\n",
            "        else:\n",
            "            ocr_rec = ocr_records[ocr_idx]\n",
            "            ocr_text = ocr_rec.text\n",
            "            ocr_time = ocr_rec.time\n",
            "            ocr_frame = ocr_rec.frame\n",
            "\n",
            "        segments_merged.append(\n",
            "            Segment(\n",
            "                start=seg.start,\n",
            "                end=seg.end,\n",
            "                mid=mid,\n",
            "                speech=seg.text,\n",
            "                slide_text=ocr_text,\n",
            "                slide_time=ocr_time,\n",
            "                slide_frame=ocr_frame,\n",
            "            )\n",
            "        )\n",
            "\n",
            "    with open(segments_path, \"w\", encoding=\"utf-8\") as f:\n",
            "        json.dump(segments_to_jsonable(segments_merged), f, ensure_ascii=False, indent=2)\n",
            "\n",
            "    print(f\"Saved aligned multimodal segments to: {segments_path}\")\n",
            "    print(f\"Total segments: {len(segments_merged)}\")\n",
            "\n",
            "    return segments_merged\n",
            "\n",
            "\n",
            "def preview_segments(segments: List[Segment], n: int = 5) -> None:\n",
            "    print(f\"Aligned segments: {len(segments)}\")\n",
            "    for seg in segments[:n]:\n",
            "        print(\n",
            "            f\"[{seg.start:.2f} -> {seg.end:.2f}] \"\n",
            "            f\"(slide at {seg.slide_time}) speech='{seg.speech[:60]}...'\"\n",
            "        )\n",
            "        print(f\"Slide text: {seg.slide_text[:200]}\")\n",
            "        print(\"-\" * 40)\n",
            "\n",
            "\n",
            "\n",
            "# asr.py\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "import json\n",
            "import os\n",
            "from typing import List\n",
            "\n",
            "from faster_whisper import WhisperModel\n",
            "\n",
            "from .models import TranscriptSegment, transcript_to_jsonable\n",
            "\n",
            "\n",
            "def run_asr(\n",
            "    audio_path: str,\n",
            "    transcript_path: str,\n",
            "    model_size: str = \"small\",\n",
            "    device: str = \"cuda\",\n",
            "    compute_type: str = \"int8\",\n",
            ") -> List[TranscriptSegment]:\n",
            "    transcript_dir = os.path.dirname(transcript_path)\n",
            "    if transcript_dir:\n",
            "        os.makedirs(transcript_dir, exist_ok=True)\n",
            "\n",
            "    if os.path.exists(transcript_path):\n",
            "        print(f\"Transcript file already exists: {transcript_path}\")\n",
            "        with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
            "            raw = json.load(f)\n",
            "        segments = [TranscriptSegment.from_dict(x) for x in raw]\n",
            "        print(f\"Loaded {len(segments)} transcript segments.\")\n",
            "        return segments\n",
            "\n",
            "    print(\"Loading faster-whisper model …\")\n",
            "    model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
            "\n",
            "    print(f\"Transcribing audio: {audio_path}\")\n",
            "    segments_iter, info = model.transcribe(str(audio_path), beam_size=5)\n",
            "\n",
            "    segments: List[TranscriptSegment] = []\n",
            "    for seg in segments_iter:\n",
            "        segments.append(\n",
            "            TranscriptSegment(\n",
            "                start=float(seg.start),\n",
            "                end=float(seg.end),\n",
            "                text=seg.text.strip(),\n",
            "            )\n",
            "        )\n",
            "\n",
            "    with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
            "        json.dump(transcript_to_jsonable(segments), f, ensure_ascii=False, indent=2)\n",
            "\n",
            "    print(f\"Saved transcript to: {transcript_path}\")\n",
            "    print(f\"Number of transcript segments: {len(segments)}\")\n",
            "\n",
            "    return segments\n",
            "\n",
            "\n",
            "def preview_transcript(segments: List[TranscriptSegment], n: int = 5) -> None:\n",
            "    print(f\"Total segments: {len(segments)}\")\n",
            "    for seg in segments[:n]:\n",
            "        print(f\"[{seg.start:.2f} -> {seg.end:.2f}] {seg.text}\")\n",
            "\n",
            "\n",
            "\n",
            "# ingest.py\n",
            "\n",
            "# src/ingest.py\n",
            "\n",
            "import os\n",
            "from moviepy import VideoFileClip\n",
            "from PIL import Image\n",
            "from tqdm import tqdm\n",
            "\n",
            "\n",
            "def inspect_video(video_path: str) -> None:\n",
            "    if not os.path.exists(video_path):\n",
            "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
            "\n",
            "    file_size_mb = os.path.getsize(video_path) / (1024 * 1024)\n",
            "\n",
            "    # Ensure the reader is closed when we are done\n",
            "    with VideoFileClip(video_path) as clip:\n",
            "        duration = clip.duration\n",
            "        fps = clip.fps\n",
            "\n",
            "    print(f\"Found video file: {os.path.basename(video_path)}\")\n",
            "    print(f\"Size: {file_size_mb:.2f} MB\")\n",
            "    print(f\"Duration: {duration:.2f} seconds ({duration / 60:.2f} minutes)\")\n",
            "    print(f\"Frame rate (fps): {fps}\")\n",
            "\n",
            "\n",
            "def extract_audio(video_path: str, audio_path: str) -> None:\n",
            "    os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n",
            "\n",
            "    if os.path.exists(audio_path):\n",
            "        print(f\"Audio file already exists: {audio_path}\")\n",
            "        return\n",
            "\n",
            "    # Open, write audio, then close\n",
            "    with VideoFileClip(video_path) as clip:\n",
            "        print(\"Extracting audio track ...\")\n",
            "        clip.audio.write_audiofile(audio_path)\n",
            "        print(f\"Saved audio to: {audio_path}\")\n",
            "\n",
            "\n",
            "def extract_frames(\n",
            "    video_path: str,\n",
            "    frame_dir: str,\n",
            "    interval_seconds: int = 3,\n",
            ") -> None:\n",
            "    os.makedirs(frame_dir, exist_ok=True)\n",
            "\n",
            "    # Open the video only inside this context\n",
            "    with VideoFileClip(video_path) as clip:\n",
            "        duration = clip.duration\n",
            "\n",
            "        n_frames = int(duration // interval_seconds) + 1\n",
            "        print(f\"Planned number of frames: {n_frames}\")\n",
            "        print(f\"Saving frames to: {frame_dir}\")\n",
            "\n",
            "        for i, t in enumerate(tqdm(range(0, int(duration) + 1, interval_seconds))):\n",
            "            frame_time = min(t, duration)\n",
            "            frame = clip.get_frame(frame_time)\n",
            "            frame_path = os.path.join(frame_dir, f\"frame_{i:05d}.jpg\")\n",
            "            if os.path.exists(frame_path):\n",
            "                continue\n",
            "            img = Image.fromarray(frame)\n",
            "            img.save(frame_path, format=\"JPEG\")\n",
            "\n",
            "    audio_path = os.path.join(os.path.dirname(frame_dir), \"audio.wav\")\n",
            "    audio_present = os.path.exists(audio_path)\n",
            "    frame_files = sorted(\n",
            "        f for f in os.listdir(frame_dir) if f.startswith(\"frame_\") and f.endswith(\".jpg\")\n",
            "    )\n",
            "\n",
            "    print(f\"Audio present: {audio_present}\")\n",
            "    print(f\"Number of frame files: {len(frame_files)}\")\n",
            "    print(\"First few frame files:\")\n",
            "    for f in frame_files[:5]:\n",
            "        print(\" -\", f)\n",
            "\n",
            "\n",
            "\n",
            "# main.py\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "import argparse\n",
            "import os\n",
            "\n",
            "from .ingest import inspect_video, extract_audio, extract_frames\n",
            "from .asr import run_asr, preview_transcript\n",
            "from .ocr import run_ocr_on_frames, preview_ocr\n",
            "from .align import align_transcript_and_ocr, preview_segments\n",
            "from .summarise import summarise_segments\n",
            "\n",
            "\n",
            "def run_full_pipeline(\n",
            "    video_path: str,\n",
            "    output_dir: str,\n",
            "    frame_interval_seconds: int = 3,\n",
            "    ocr_frame_stride: int = 2,\n",
            "    device_summariser: int = 0,\n",
            ") -> None:\n",
            "    os.makedirs(output_dir, exist_ok=True)\n",
            "    frame_dir = os.path.join(output_dir, \"frames\")\n",
            "    os.makedirs(frame_dir, exist_ok=True)\n",
            "\n",
            "    audio_path = os.path.join(output_dir, \"audio.wav\")\n",
            "    transcript_path = os.path.join(output_dir, \"transcript_segments.json\")\n",
            "    ocr_output_path = os.path.join(output_dir, \"ocr_frames.json\")\n",
            "    segments_path = os.path.join(output_dir, \"segments.json\")\n",
            "\n",
            "    print(\"=== Inspect video ===\")\n",
            "    inspect_video(video_path)\n",
            "\n",
            "    print(\"\\n=== Extract audio ===\")\n",
            "    extract_audio(video_path, audio_path)\n",
            "\n",
            "    print(\"\\n=== Extract frames ===\")\n",
            "    extract_frames(video_path, frame_dir, interval_seconds=frame_interval_seconds)\n",
            "\n",
            "    print(\"\\n=== ASR ===\")\n",
            "    transcript_segments = run_asr(audio_path, transcript_path)\n",
            "    preview_transcript(transcript_segments, n=5)\n",
            "\n",
            "    print(\"\\n=== OCR ===\")\n",
            "    ocr_records = run_ocr_on_frames(\n",
            "        frame_dir=frame_dir,\n",
            "        ocr_output_path=ocr_output_path,\n",
            "        frame_interval_seconds=frame_interval_seconds,\n",
            "        ocr_frame_stride=ocr_frame_stride,\n",
            "    )\n",
            "    preview_ocr(ocr_records, n=5)\n",
            "\n",
            "    print(\"\\n=== Alignment ===\")\n",
            "    segments_merged = align_transcript_and_ocr(\n",
            "        transcript_segments=transcript_segments,\n",
            "        ocr_records=ocr_records,\n",
            "        segments_path=segments_path,\n",
            "    )\n",
            "    preview_segments(segments_merged, n=5)\n",
            "\n",
            "    print(\"\\n=== Global summarisation ===\")\n",
            "    summarise_segments(\n",
            "        segments_merged,\n",
            "        model_name=\"facebook/bart-large-cnn\",\n",
            "        device=device_summariser,\n",
            "        max_chunk_chars=3000,\n",
            "        max_length=500,\n",
            "        min_length=40,\n",
            "    )\n",
            "\n",
            "    print(\"\\nPipeline completed.\")\n",
            "\n",
            "\n",
            "def main() -> None:\n",
            "    parser = argparse.ArgumentParser(\n",
            "        description=\"Multimodal NLP pipeline: video → audio, frames, ASR, OCR, alignment, summary.\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--video-path\",\n",
            "        type=str,\n",
            "        required=True,\n",
            "        help=\"Path to input MP4 video.\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--output-dir\",\n",
            "        type=str,\n",
            "        required=True,\n",
            "        help=\"Directory where artefacts will be written.\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--frame-interval-seconds\",\n",
            "        type=int,\n",
            "        default=3,\n",
            "        help=\"Interval in seconds between extracted frames.\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--ocr-frame-stride\",\n",
            "        type=int,\n",
            "        default=2,\n",
            "        help=\"Stride for selecting frames for OCR (every nth frame).\",\n",
            "    )\n",
            "    parser.add_argument(\n",
            "        \"--summariser-device\",\n",
            "        type=int,\n",
            "        default=0,\n",
            "        help=\"Device index for the summarisation model (use -1 for CPU).\",\n",
            "    )\n",
            "\n",
            "    args = parser.parse_args()\n",
            "\n",
            "    run_full_pipeline(\n",
            "        video_path=args.video_path,\n",
            "        output_dir=args.output_dir,\n",
            "        frame_interval_seconds=args.frame_interval_seconds,\n",
            "        ocr_frame_stride=args.ocr_frame_stride,\n",
            "        device_summariser=args.summariser_device,\n",
            "    )\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "\n",
            "\n",
            "\n",
            "# models.py\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "from dataclasses import dataclass, asdict\n",
            "from typing import Optional, List, Dict, Any\n",
            "\n",
            "\n",
            "@dataclass\n",
            "class TranscriptSegment:\n",
            "    start: float\n",
            "    end: float\n",
            "    text: str\n",
            "\n",
            "    def to_dict(self) -> Dict[str, Any]:\n",
            "        return asdict(self)\n",
            "\n",
            "    @staticmethod\n",
            "    def from_dict(data: Dict[str, Any]) -> \"TranscriptSegment\":\n",
            "        return TranscriptSegment(\n",
            "            start=float(data[\"start\"]),\n",
            "            end=float(data[\"end\"]),\n",
            "            text=str(data[\"text\"]),\n",
            "        )\n",
            "\n",
            "\n",
            "@dataclass\n",
            "class OCRRecord:\n",
            "    time: float\n",
            "    frame: str\n",
            "    text: str\n",
            "\n",
            "    def to_dict(self) -> Dict[str, Any]:\n",
            "        return asdict(self)\n",
            "\n",
            "    @staticmethod\n",
            "    def from_dict(data: Dict[str, Any]) -> \"OCRRecord\":\n",
            "        return OCRRecord(\n",
            "            time=float(data[\"time\"]),\n",
            "            frame=str(data[\"frame\"]),\n",
            "            text=str(data[\"text\"]),\n",
            "        )\n",
            "\n",
            "\n",
            "@dataclass\n",
            "class Segment:\n",
            "    start: float\n",
            "    end: float\n",
            "    mid: float\n",
            "    speech: str\n",
            "    slide_text: str\n",
            "    slide_time: Optional[float]\n",
            "    slide_frame: Optional[str]\n",
            "\n",
            "    def to_dict(self) -> Dict[str, Any]:\n",
            "        return asdict(self)\n",
            "\n",
            "    @staticmethod\n",
            "    def from_dict(data: Dict[str, Any]) -> \"Segment\":\n",
            "        return Segment(\n",
            "            start=float(data[\"start\"]),\n",
            "            end=float(data[\"end\"]),\n",
            "            mid=float(data[\"mid\"]),\n",
            "            speech=str(data[\"speech\"]),\n",
            "            slide_text=str(data.get(\"slide_text\", \"\")),\n",
            "            slide_time=data.get(\"slide_time\"),\n",
            "            slide_frame=data.get(\"slide_frame\"),\n",
            "        )\n",
            "\n",
            "\n",
            "def segments_to_jsonable(items: List[Segment]) -> List[Dict[str, Any]]:\n",
            "    return [item.to_dict() for item in items]\n",
            "\n",
            "\n",
            "def transcript_to_jsonable(items: List[TranscriptSegment]) -> List[Dict[str, Any]]:\n",
            "    return [item.to_dict() for item in items]\n",
            "\n",
            "\n",
            "def ocr_to_jsonable(items: List[OCRRecord]) -> List[Dict[str, Any]]:\n",
            "    return [item.to_dict() for item in items]\n",
            "\n",
            "\n",
            "\n",
            "# ocr.py\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "import json\n",
            "import os\n",
            "import glob\n",
            "from typing import List\n",
            "\n",
            "from PIL import Image\n",
            "import pytesseract\n",
            "from tqdm import tqdm\n",
            "\n",
            "from .models import OCRRecord, ocr_to_jsonable\n",
            "\n",
            "\n",
            "def run_ocr_on_frames(\n",
            "    frame_dir: str,\n",
            "    ocr_output_path: str,\n",
            "    frame_interval_seconds: int = 3,\n",
            "    ocr_frame_stride: int = 2,\n",
            ") -> List[OCRRecord]:\n",
            "    \"\"\"\n",
            "    Runs OCR on sampled frames and stores the result.\n",
            "    The `frame_interval_seconds` must match the interval used when extracting frames.\n",
            "    \"\"\"\n",
            "    ocr_output_dir = os.path.dirname(ocr_output_path)\n",
            "    if ocr_output_dir:\n",
            "        os.makedirs(ocr_output_dir, exist_ok=True)\n",
            "\n",
            "    if os.path.exists(ocr_output_path):\n",
            "        print(f\"OCR output already exists: {ocr_output_path}\")\n",
            "        with open(ocr_output_path, \"r\", encoding=\"utf-8\") as f:\n",
            "            raw = json.load(f)\n",
            "        records = [OCRRecord.from_dict(x) for x in raw]\n",
            "        print(f\"Loaded OCR text for {len(records)} frames.\")\n",
            "        return records\n",
            "\n",
            "    records: List[OCRRecord] = []\n",
            "    print(\"Running OCR on sampled frames …\")\n",
            "\n",
            "    frame_files = sorted(glob.glob(os.path.join(frame_dir, \"frame_*.jpg\")))\n",
            "\n",
            "    for idx, frame_path in enumerate(tqdm(frame_files)):\n",
            "        if idx % ocr_frame_stride != 0:\n",
            "            continue\n",
            "\n",
            "        approx_time = idx * frame_interval_seconds\n",
            "\n",
            "        with Image.open(frame_path) as img:\n",
            "            text = pytesseract.image_to_string(img)\n",
            "\n",
            "        records.append(\n",
            "            OCRRecord(\n",
            "                time=float(approx_time),\n",
            "                frame=os.path.basename(frame_path),\n",
            "                text=text.strip(),\n",
            "            )\n",
            "        )\n",
            "\n",
            "    with open(ocr_output_path, \"w\", encoding=\"utf-8\") as f:\n",
            "        json.dump(ocr_to_jsonable(records), f, ensure_ascii=False, indent=2)\n",
            "\n",
            "    print(f\"Saved OCR output for {len(records)} frames to: {ocr_output_path}\")\n",
            "    return records\n",
            "\n",
            "\n",
            "def preview_ocr(records: List[OCRRecord], n: int = 5) -> None:\n",
            "    print(f\"OCR records: {len(records)}\")\n",
            "    for rec in records[:n]:\n",
            "        print(f\"[t ~ {rec.time:.1f}s] frame={rec.frame}\")\n",
            "        print(rec.text)\n",
            "        print(\"-\" * 40)\n",
            "\n",
            "\n",
            "\n",
            "# summarise.py\n",
            "\n",
            "from __future__ import annotations\n",
            "\n",
            "from typing import List\n",
            "\n",
            "from transformers import pipeline\n",
            "\n",
            "from .models import Segment\n",
            "\n",
            "\n",
            "def _chunk_text(text: str, max_chars: int) -> List[str]:\n",
            "    chunks: List[str] = []\n",
            "    current: List[str] = []\n",
            "    current_len = 0\n",
            "    for line in text.split(\"\\n\"):\n",
            "        line_len = len(line) + 1\n",
            "        if current_len + line_len > max_chars and current:\n",
            "            chunks.append(\"\\n\".join(current))\n",
            "            current = [line]\n",
            "            current_len = line_len\n",
            "        else:\n",
            "            current.append(line)\n",
            "            current_len += line_len\n",
            "    if current:\n",
            "        chunks.append(\"\\n\".join(current))\n",
            "    return chunks\n",
            "\n",
            "\n",
            "def summarise_segments(\n",
            "    segments: List[Segment],\n",
            "    model_name: str = \"facebook/bart-large-cnn\",\n",
            "    device: int = 0,\n",
            "    max_chunk_chars: int = 3000,\n",
            "    max_length: int = 500,\n",
            "    min_length: int = 40,\n",
            ") -> str:\n",
            "    full_transcript_text = \"\\n\".join(seg.speech for seg in segments)\n",
            "    print(f\"Total transcript length (characters): {len(full_transcript_text)}\")\n",
            "\n",
            "    chunks = _chunk_text(full_transcript_text, max_chunk_chars)\n",
            "    print(f\"Number of chunks for summarisation: {len(chunks)}\")\n",
            "\n",
            "    summariser = pipeline(\n",
            "        \"summarization\",\n",
            "        model=model_name,\n",
            "        device=device,  # use -1 for CPU\n",
            "    )\n",
            "\n",
            "    summaries: List[str] = []\n",
            "    for idx, ch in enumerate(chunks):\n",
            "        print(f\"Summarising chunk {idx + 1}/{len(chunks)} …\")\n",
            "        out = summariser(\n",
            "            ch,\n",
            "            max_length=max_length,\n",
            "            min_length=min_length,\n",
            "            do_sample=False,\n",
            "        )\n",
            "        summaries.append(out[0][\"summary_text\"])\n",
            "\n",
            "    global_summary = \"\\n\".join(summaries)\n",
            "\n",
            "    print(\"\\n=== GLOBAL SUMMARY ===\\n\")\n",
            "    print(global_summary)\n",
            "\n",
            "    return global_summary\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "src_dir = Path(\"src\")\n",
        "\n",
        "for script_path in sorted(src_dir.glob(\"*.py\")):\n",
        "    print(f\"# {script_path.name}\\n\")\n",
        "    with script_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        print(f.read())\n",
        "        print(\"\\n\")  # extra newline between files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f3031d",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
